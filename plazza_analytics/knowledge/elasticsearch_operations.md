# Elasticsearch Operations Knowledge Base - 2025-03-26

## Metadata
- **Documentation Date**: 2025-03-26
- **Documentation Type**: Technical
- **Data Domains**: Elasticsearch, Search, Indexing, Queries
- **Generated By**: Claude Code

## Elasticsearch Connection and Setup

### Connection Setup
```python
from elasticsearch import Elasticsearch
import os

# Connect to Elasticsearch
es = Elasticsearch(
    os.getenv('ES_HOST', 'https://search.plazza.in'),
    basic_auth=(os.getenv('ES_USER', 'elastic'), os.getenv('ES_PASSWORD')),
    verify_certs=False  # For development only - enable cert verification in production
)

# Check connection
if es.ping():
    print("Connected to Elasticsearch")
else:
    print("Could not connect to Elasticsearch")
```

### Index Creation with Custom Mappings
```python
# Define mapping for product catalog
def create_product_index():
    # Define index mapping with optimized settings
    mapping = {
        "settings": {
            "number_of_shards": 3,
            "number_of_replicas": 1,
            "analysis": {
                "normalizer": {
                    "lowercase_normalizer": {
                        "type": "custom",
                        "filter": ["lowercase"]
                    }
                },
                "analyzer": {
                    "product_analyzer": {
                        "type": "custom",
                        "tokenizer": "standard",
                        "filter": ["lowercase", "asciifolding"]
                    }
                }
            }
        },
        "mappings": {
            "properties": {
                "product_id": {"type": "keyword"},
                "name": {
                    "type": "text",
                    "analyzer": "product_analyzer",
                    "fields": {
                        "keyword": {
                            "type": "keyword",
                            "normalizer": "lowercase_normalizer"
                        }
                    }
                },
                "normalized_name": {"type": "keyword"},
                "manufacturers": {"type": "text", "fields": {"keyword": {"type": "keyword"}}},
                "mrp": {"type": "float"},
                "selling_price": {"type": "float"},
                "package_detail": {"type": "text"},
                "prescription_required": {"type": "boolean"},
                "salt_composition": {"type": "text"},
                "product_suggest": {
                    "type": "completion",
                    "analyzer": "product_analyzer"
                },
                "created_at": {"type": "date"},
                "updated_at": {"type": "date"}
            }
        }
    }
    
    # Create the index
    if not es.indices.exists(index="plazza-catalogue"):
        es.indices.create(index="plazza-catalogue", body=mapping)
        print("Created index: plazza-catalogue")
    else:
        print("Index already exists: plazza-catalogue")
```

### Distributor Index Creation
```python
# Create index for distributor data with advanced text analysis
def create_distributor_index():
    mapping = {
        "settings": {
            "number_of_shards": 3,
            "number_of_replicas": 1,
            "analysis": {
                "normalizer": {
                    "lowercase_normalizer": {
                        "type": "custom",
                        "filter": ["lowercase"]
                    }
                },
                "analyzer": {
                    "product_analyzer": {
                        "type": "custom",
                        "tokenizer": "standard",
                        "filter": ["lowercase", "asciifolding"]
                    }
                }
            }
        },
        "mappings": {
            "properties": {
                "id": {"type": "keyword"},
                "product_id": {"type": "keyword"},
                "product_name": {
                    "type": "text",
                    "analyzer": "product_analyzer",
                    "fields": {
                        "keyword": {
                            "type": "keyword",
                            "normalizer": "lowercase_normalizer"
                        }
                    }
                },
                "normalized_name": {"type": "keyword"},
                "manufacturer": {"type": "text", "fields": {"keyword": {"type": "keyword"}}},
                "item_code": {"type": "keyword"},
                "original_item_code": {"type": "keyword"},
                "hsn_code": {"type": "keyword"},
                "package": {"type": "text"},
                "distributor": {"type": "keyword"},
                "mrp": {"type": "scaled_float", "scaling_factor": 100},
                "purchase_rate": {"type": "scaled_float", "scaling_factor": 100},
                "gst_rate": {"type": "scaled_float", "scaling_factor": 100},
                "plazza_selling_price_incl_gst": {"type": "scaled_float", "scaling_factor": 100},
                "effective_customer_discount": {"type": "scaled_float", "scaling_factor": 100},
                "product_suggest": {
                    "type": "completion",
                    "analyzer": "product_analyzer"
                },
                "created_at": {"type": "date"},
                "updated_at": {"type": "date"}
            }
        }
    }
    
    if not es.indices.exists(index="distributor-master-list"):
        es.indices.create(index="distributor-master-list", body=mapping)
        print("Created index: distributor-master-list")
    else:
        print("Index already exists: distributor-master-list")
```

## Document Management

### Single Document Indexing
```python
# Index a single document
def index_product(product):
    doc = {
        'product_id': product['product_id'],
        'name': product['name'],
        'normalized_name': normalize_product_name(product['name']),
        'manufacturers': product['manufacturers'],
        'mrp': float(product['mrp']),
        'selling_price': float(product['selling_price']),
        'package_detail': product.get('package_detail', ''),
        'prescription_required': product.get('prescription_required', False),
        'salt_composition': product.get('salt_composition', ''),
        'product_suggest': {
            'input': [product['name']]
        },
        'created_at': datetime.now().isoformat(),
        'updated_at': datetime.now().isoformat()
    }
    
    # Index the document
    result = es.index(index="plazza-catalogue", id=product['product_id'], document=doc)
    return result
```

### Bulk Document Indexing
```python
from elasticsearch import helpers
import time

# Process documents in batches for efficiency
def bulk_index_products(products, index_name="plazza-catalogue", batch_size=1000):
    total_docs = len(products)
    processed = 0
    start_time = time.time()
    
    # Process in batches
    for i in range(0, total_docs, batch_size):
        batch = products[i:i+batch_size]
        actions = []
        
        for product in batch:
            # Transform the document
            doc = transform_product_to_es_doc(product)
            
            # Create action
            action = {
                '_index': index_name,
                '_id': product['product_id'],
                '_source': doc
            }
            actions.append(action)
        
        # Execute bulk indexing
        success, errors = helpers.bulk(es, actions, stats_only=True)
        processed += success
        
        # Calculate progress and speed
        elapsed = time.time() - start_time
        docs_per_second = processed / elapsed if elapsed > 0 else 0
        progress = (processed / total_docs) * 100 if total_docs > 0 else 0
        
        print(f"Processed {processed}/{total_docs} documents "
              f"({progress:.1f}%) - {docs_per_second:.1f} docs/sec")
        
        if errors:
            print(f"Encountered {errors} errors in this batch")
    
    total_time = time.time() - start_time
    print(f"Indexing complete. Processed {processed} documents in {total_time:.2f} seconds "
          f"({processed/total_time:.1f} docs/sec)")
    
    return processed
```

### Partial Document Updates
```python
# Update specific fields without overwriting the entire document
def update_product_fields(product_id, fields_to_update):
    # Prepare the update body
    update_body = {
        "doc": fields_to_update,
        "doc_as_upsert": False  # Don't create if document doesn't exist
    }
    
    try:
        result = es.update(
            index="plazza-catalogue",
            id=product_id,
            body=update_body
        )
        return result
    except Exception as e:
        print(f"Error updating product {product_id}: {str(e)}")
        return None
```

### Deleting Documents
```python
# Delete documents matching a query
def delete_documents_by_query(index_name, query):
    result = es.delete_by_query(
        index=index_name,
        body={"query": query},
        conflicts="proceed"  # Continue even if there are version conflicts
    )
    
    print(f"Deleted {result['deleted']} documents")
    if result['failures']:
        print(f"Had {len(result['failures'])} failures")
    
    return result
```

## Search Query Construction

### Multi-Field Product Search
```python
# Comprehensive product search with multiple fields
def search_products(query_text, 
                    filters=None, 
                    sort_by=None, 
                    page=0, 
                    size=20,
                    fuzziness="AUTO"):
    """
    Search for products with a comprehensive query that searches across multiple fields
    with boosted relevance on important fields.
    
    Parameters:
    - query_text: The search text from the user
    - filters: Dictionary of field:value pairs to filter results
    - sort_by: Optional field to sort by
    - page: Page number (0-based)
    - size: Results per page
    - fuzziness: Fuzziness level for the search (AUTO, 0, 1, 2)
    """
    # Build query components
    should_clauses = [
        # Exact matches on name get highest boost
        {"match_phrase": {"name": {"query": query_text, "boost": 10.0}}},
        
        # Standard text search on name with fuzziness
        {"match": {"name": {
            "query": query_text, 
            "fuzziness": fuzziness,
            "boost": 5.0
        }}},
        
        # Lower boost for manufacturer matches
        {"match": {"manufacturers": {
            "query": query_text, 
            "fuzziness": fuzziness,
            "boost": 2.0
        }}},
        
        # Salt composition with lower boost
        {"match": {"salt_composition": {
            "query": query_text, 
            "boost": 1.0
        }}}
    ]
    
    # Add wildcard for partial matches (prefix and suffix)
    should_clauses.append({
        "wildcard": {"name.keyword": {
            "value": f"*{query_text.lower()}*", 
            "boost": 3.0
        }}
    })
    
    # Build the complete query
    query_body = {
        "query": {
            "bool": {
                "should": should_clauses,
                "minimum_should_match": 1
            }
        },
        "size": size,
        "from": page * size
    }
    
    # Add filters if provided
    if filters:
        filter_clauses = []
        for field, value in filters.items():
            if isinstance(value, list):
                filter_clauses.append({"terms": {field: value}})
            elif isinstance(value, dict) and ('from' in value or 'to' in value):
                range_filter = {"range": {field: {}}}
                if 'from' in value:
                    range_filter["range"][field]["gte"] = value['from']
                if 'to' in value:
                    range_filter["range"][field]["lte"] = value['to']
                filter_clauses.append(range_filter)
            else:
                filter_clauses.append({"term": {field: value}})
        
        query_body["query"]["bool"]["filter"] = filter_clauses
    
    # Add sorting if requested
    if sort_by:
        if isinstance(sort_by, list):
            query_body["sort"] = sort_by
        else:
            query_body["sort"] = [sort_by]
    
    # Execute the search
    results = es.search(
        index="plazza-catalogue",
        body=query_body
    )
    
    return results
```

### Autocomplete and Suggestion Queries
```python
# Get autocomplete suggestions for product search
def get_product_suggestions(prefix, size=10):
    """
    Get autocomplete suggestions for product search using the completion suggester.
    
    Parameters:
    - prefix: The prefix text to get suggestions for
    - size: Number of suggestions to return
    """
    suggestion_query = {
        "suggest": {
            "product_suggestion": {
                "prefix": prefix,
                "completion": {
                    "field": "product_suggest",
                    "size": size,
                    "skip_duplicates": True,
                    "fuzzy": {
                        "fuzziness": 1
                    }
                }
            }
        },
        "_source": ["product_id", "name", "manufacturers", "mrp", "selling_price"]
    }
    
    results = es.search(
        index="plazza-catalogue",
        body=suggestion_query
    )
    
    # Extract and format suggestions
    suggestions = []
    if 'suggest' in results and 'product_suggestion' in results['suggest']:
        for suggestion_group in results['suggest']['product_suggestion']:
            for option in suggestion_group['options']:
                suggestions.append({
                    'product_id': option['_source']['product_id'],
                    'name': option['_source']['name'],
                    'manufacturer': option['_source'].get('manufacturers', ''),
                    'mrp': option['_source'].get('mrp', 0),
                    'text': option['text'],
                    'score': option['_score']
                })
    
    return suggestions
```

### Advanced Query Construction for Product Matching
```python
# Build sophisticated match queries for product matching
def build_product_match_query(product_name, mrp=None, manufacturer=None):
    """
    Build a sophisticated query for matching products between systems.
    
    This combines multiple query approaches for optimal matching:
    1. Exact phrase matching with highest priority
    2. Text matching with fuzziness for typos
    3. Token set matching for word order variations
    4. Individual token matching for partial matches
    """
    # Normalize the product name
    normalized_name = normalize_product_name(product_name)
    
    # Build core search clauses
    should_clauses = [
        # Exact phrase matches (highest relevance)
        {"match_phrase": {"name": {"query": normalized_name, "boost": 10.0}}},
        
        # Standard text search with fuzziness for typos
        {"match": {"name": {"query": normalized_name, "fuzziness": "AUTO", "boost": 5.0}}},
        
        # Normalized name for standardized matching
        {"match": {"normalized_name": {"query": normalized_name, "boost": 7.0}}},
        
        # Wildcard for contains matches
        {"wildcard": {"normalized_name": {"value": f"*{normalized_name}*", "boost": 3.0}}}
    ]
    
    # Add individual token matching for partial matches
    tokens = normalized_name.split()
    significant_tokens = [token for token in tokens if len(token) > 3]
    
    for token in significant_tokens:
        should_clauses.append({
            "match": {"name": {"query": token, "boost": 0.5}}
        })
    
    # Build final query with score boosting
    query = {
        "query": {
            "bool": {
                "should": should_clauses,
                "minimum_should_match": 1
            }
        },
        "size": 50  # Return top 50 potential matches
    }
    
    # Add MRP filter if provided (with tolerance)
    if mrp is not None and mrp > 0:
        mrp_tolerance = 0.1  # 10% tolerance
        query["query"]["bool"]["filter"] = [{
            "range": {
                "mrp": {
                    "gte": mrp * (1 - mrp_tolerance),
                    "lte": mrp * (1 + mrp_tolerance)
                }
            }
        }]
    
    # Add manufacturer boost if provided
    if manufacturer:
        query["query"]["bool"]["should"].append({
            "match": {
                "manufacturers": {
                    "query": manufacturer,
                    "boost": 2.0  # Important but not decisive
                }
            }
        })
    
    return query
```

## Advanced Search Techniques

### Fuzzy Product Name Matching
```python
# Normalize product names for better matching
def normalize_product_name(name):
    """
    Normalize a product name to improve matching between systems.
    
    This function:
    1. Converts to lowercase
    2. Removes dosage/strength patterns (10mg, 500ml, etc.)
    3. Removes packaging information (10x10, 10's, etc.)
    4. Removes common prefixes (tab, cap, inj, etc.)
    5. Removes special characters and extra spaces
    """
    if not name:
        return ""
    
    # Convert to lowercase
    name = name.lower()
    
    # Remove dosage/strength patterns
    name = re.sub(r'\b\d+\s*([a-z]{1,3})\b', '', name)  # 10mg, 500ml, etc.
    
    # Remove packaging information
    name = re.sub(r'\b\d+\s*x\s*\d+\b', '', name)  # 10x10
    name = re.sub(r'\b\d+\s*\'?s\b', '', name)     # 10's
    
    # Remove common prefixes
    prefixes = ['tab', 'cap', 'inj', 'syp', 'oint', 'cream', 'lotion', 'drops']
    words = name.split()
    if words and words[0] in prefixes:
        words = words[1:]
    name = ' '.join(words)
    
    # Remove special characters except spaces
    name = re.sub(r'[^\w\s]', '', name)
    
    # Remove extra spaces
    name = re.sub(r'\s+', ' ', name).strip()
    
    return name
```

### Paginated Search with the Scroll API
```python
# Paginate through large result sets efficiently
def scroll_all_documents(index_name, query=None, batch_size=1000, scroll_timeout="5m"):
    """
    Efficiently retrieve all documents that match a query using the scroll API.
    
    Parameters:
    - index_name: The name of the index to search
    - query: Optional query to filter documents (None for all documents)
    - batch_size: Number of documents per batch
    - scroll_timeout: How long to keep the scroll context alive
    
    Yields batches of documents.
    """
    # Initial search query
    body = {
        "size": batch_size,
        "query": query if query else {"match_all": {}}
    }
    
    # Get scroll ID
    response = es.search(
        index=index_name,
        body=body,
        scroll=scroll_timeout
    )
    
    # Get the scroll ID
    scroll_id = response["_scroll_id"]
    
    # Get total number of documents
    total_docs = response["hits"]["total"]["value"]
    print(f"Found {total_docs} documents, retrieving in batches of {batch_size}")
    
    # Process results
    hits = response["hits"]["hits"]
    documents_processed = len(hits)
    
    # Return first batch
    if hits:
        yield hits
    
    # Continue scrolling until no more hits
    while hits:
        # Get next batch using the scroll ID
        response = es.scroll(
            scroll_id=scroll_id,
            scroll=scroll_timeout
        )
        
        # Get next batch of hits
        hits = response["hits"]["hits"]
        documents_processed += len(hits)
        
        # Print progress
        if hits:
            progress = (documents_processed / total_docs) * 100
            print(f"Processed {documents_processed}/{total_docs} documents ({progress:.1f}%)")
            yield hits
    
    # Clear the scroll context
    es.clear_scroll(scroll_id=scroll_id)
    print(f"Scroll complete, processed {documents_processed} documents")
```

### Multi-Field Aggregation Queries
```python
# Get insights via aggregations on multiple fields
def get_product_price_distribution(manufacturer=None, category=None):
    """
    Get distribution of product prices with multiple aggregations.
    
    Parameters:
    - manufacturer: Optional filter for specific manufacturer
    - category: Optional filter for specific category
    """
    # Build query with filters
    query = {"bool": {"must": []}}
    
    if manufacturer:
        query["bool"]["must"].append({
            "match": {"manufacturers.keyword": manufacturer}
        })
    
    if category:
        query["bool"]["must"].append({
            "match": {"category.keyword": category}
        })
    
    # Build aggregations
    aggs = {
        "price_ranges": {
            "range": {
                "field": "mrp",
                "ranges": [
                    {"to": 100},
                    {"from": 100, "to": 500},
                    {"from": 500, "to": 1000},
                    {"from": 1000, "to": 2000},
                    {"from": 2000}
                ]
            }
        },
        "avg_price": {"avg": {"field": "mrp"}},
        "max_price": {"max": {"field": "mrp"}},
        "min_price": {"min": {"field": "mrp"}},
        "price_percentiles": {
            "percentiles": {
                "field": "mrp",
                "percents": [10, 25, 50, 75, 90, 95, 99]
            }
        },
        # Add sub-aggregations for manufacturers within each price range
        "manufacturers": {
            "terms": {
                "field": "manufacturers.keyword",
                "size": 20
            },
            "aggs": {
                "avg_price": {"avg": {"field": "mrp"}}
            }
        }
    }
    
    # Build complete request
    body = {
        "size": 0,  # We only want aggregations, not actual documents
        "query": query,
        "aggs": aggs
    }
    
    # Execute query
    results = es.search(
        index="plazza-catalogue",
        body=body
    )
    
    return results["aggregations"]
```

## Data Synchronization Patterns

### Database to Elasticsearch Synchronization
```python
import psycopg2
import psycopg2.extras
import os
from elasticsearch import Elasticsearch, helpers
from datetime import datetime, timedelta
import time

def sync_products_to_elasticsearch(batch_size=1000, since=None):
    """
    Synchronize product data from CockroachDB to Elasticsearch.
    
    Parameters:
    - batch_size: Number of records to process in each batch
    - since: Optional datetime to only sync records updated since then
    """
    # Connect to database
    db_conn = psycopg2.connect(os.getenv('DATABASE_URL'))
    db_conn.autocommit = True
    
    # Connect to Elasticsearch
    es = Elasticsearch(
        os.getenv('ES_HOST'),
        basic_auth=(os.getenv('ES_USER'), os.getenv('ES_PASSWORD')),
        verify_certs=False
    )
    
    # Build query for fetching products
    if since:
        query = """
            SELECT * FROM all_products 
            WHERE updated_at > %s
            ORDER BY updated_at
        """
        params = (since,)
    else:
        query = "SELECT * FROM all_products ORDER BY updated_at"
        params = None
    
    # Get total count for progress tracking
    with db_conn.cursor() as count_cursor:
        count_query = "SELECT COUNT(*) FROM all_products"
        if params:
            count_query += " WHERE updated_at > %s"
        count_cursor.execute(count_query, params)
        total_count = count_cursor.fetchone()[0]
    
    print(f"Found {total_count} products to synchronize")
    
    # Process in batches
    with db_conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cursor:
        cursor.execute(query, params)
        
        processed = 0
        start_time = time.time()
        
        while True:
            # Fetch batch
            batch = cursor.fetchmany(batch_size)
            if not batch:
                break
            
            # Prepare actions for bulk indexing
            actions = []
            for product in batch:
                # Transform database record to ES document
                doc = {
                    'product_id': product['product_id'],
                    'name': product['product_name'],
                    'normalized_name': normalize_product_name(product['product_name']),
                    'manufacturers': product['manufacturers'],
                    'mrp': float(product['mrp']) if product['mrp'] else 0,
                    'selling_price': float(product['selling_price']) if product['selling_price'] else 0,
                    'package_detail': product.get('packaging_detail', ''),
                    'prescription_required': product.get('prescription_required', False),
                    'salt_composition': product.get('salt_composition', ''),
                    'product_suggest': {
                        'input': [product['product_name']]
                    },
                    'updated_at': product['updated_at'].isoformat() if product['updated_at'] else datetime.now().isoformat()
                }
                
                # Add to actions
                action = {
                    '_index': 'plazza-catalogue',
                    '_id': product['product_id'],
                    '_source': doc
                }
                actions.append(action)
            
            # Execute bulk indexing
            if actions:
                success, errors = helpers.bulk(es, actions, stats_only=True)
                processed += success
                
                # Calculate progress and speed
                elapsed = time.time() - start_time
                docs_per_second = processed / elapsed if elapsed > 0 else 0
                progress = (processed / total_count) * 100 if total_count > 0 else 0
                
                print(f"Processed {processed}/{total_count} products "
                      f"({progress:.1f}%) - {docs_per_second:.1f} docs/sec")
                
                if errors:
                    print(f"Encountered {errors} errors in this batch")
    
    # Clean up
    db_conn.close()
    
    # Return results
    total_time = time.time() - start_time
    print(f"Sync complete. Processed {processed} products in {total_time:.2f} seconds "
          f"({processed/total_time:.1f} docs/sec)")
    
    return {
        'total': total_count,
        'processed': processed,
        'elapsed_seconds': total_time
    }
```

### Field-Specific Synchronization
```python
def sync_specific_fields_to_elasticsearch(field_list, batch_size=1000):
    """
    Synchronize specific fields from database to Elasticsearch without 
    overwriting other fields.
    
    Parameters:
    - field_list: List of field names to synchronize
    - batch_size: Number of records to process in each batch
    """
    # Connect to database
    db_conn = psycopg2.connect(os.getenv('DATABASE_URL'))
    db_conn.autocommit = True
    
    # Connect to Elasticsearch
    es = Elasticsearch(
        os.getenv('ES_HOST'),
        basic_auth=(os.getenv('ES_USER'), os.getenv('ES_PASSWORD')),
        verify_certs=False
    )
    
    # Build query - only select needed fields
    field_sql = "product_id, " + ", ".join(field_list)
    query = f"SELECT {field_sql} FROM all_products ORDER BY updated_at"
    
    # Get total count for progress tracking
    with db_conn.cursor() as count_cursor:
        count_cursor.execute("SELECT COUNT(*) FROM all_products")
        total_count = count_cursor.fetchone()[0]
    
    print(f"Found {total_count} products to synchronize fields: {', '.join(field_list)}")
    
    # Process in batches
    with db_conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cursor:
        cursor.execute(query)
        
        processed = 0
        start_time = time.time()
        
        while True:
            # Fetch batch
            batch = cursor.fetchmany(batch_size)
            if not batch:
                break
            
            # Process batch
            for product in batch:
                # Extract product_id (required for document identification)
                product_id = product.pop('product_id')
                
                # Prepare update document (only specified fields)
                doc = {}
                for field in field_list:
                    if field in product and product[field] is not None:
                        # Transform field if needed
                        if field == 'prescription_required':
                            doc[field] = bool(product[field])
                        elif field in ['mrp', 'selling_price']:
                            doc[field] = float(product[field]) if product[field] else 0
                        else:
                            doc[field] = product[field]
                
                # Add timestamp
                doc['updated_at'] = datetime.now().isoformat()
                
                # Update document
                try:
                    es.update(
                        index='plazza-catalogue',
                        id=product_id,
                        body={"doc": doc, "doc_as_upsert": False}
                    )
                    processed += 1
                except Exception as e:
                    print(f"Error updating product {product_id}: {str(e)}")
                
                # Show progress periodically
                if processed % 100 == 0:
                    elapsed = time.time() - start_time
                    docs_per_second = processed / elapsed if elapsed > 0 else 0
                    progress = (processed / total_count) * 100 if total_count > 0 else 0
                    print(f"Processed {processed}/{total_count} products "
                          f"({progress:.1f}%) - {docs_per_second:.1f} docs/sec")
    
    # Clean up
    db_conn.close()
    
    # Return results
    total_time = time.time() - start_time
    print(f"Field sync complete. Processed {processed} products in {total_time:.2f} seconds "
          f"({processed/total_time:.1f} docs/sec)")
    
    return {
        'total': total_count,
        'processed': processed,
        'elapsed_seconds': total_time,
        'fields_updated': field_list
    }
```

## Elasticsearch Optimization and Maintenance

### Index Status and Health Monitoring
```python
def check_indices_health():
    """
    Check the health and status of all indices.
    """
    # Get index health status
    health = es.cat.health(format="json")
    print(f"Cluster health: {health[0]['status']}")
    
    # Get all indices
    indices = es.cat.indices(format="json")
    
    # Print index details
    print("\nIndex Status:")
    print("-" * 80)
    print(f"{'Index Name':<30} {'Status':<10} {'Docs':<10} {'Size':<10} {'Shards':<6}")
    print("-" * 80)
    
    for idx in indices:
        print(f"{idx['index']:<30} {idx['health']:<10} {idx['docs.count']:<10} "
              f"{idx['store.size']:<10} {idx['pri']:<6}")
    
    # Get index stats
    stats = es.indices.stats()
    
    return {
        'cluster_health': health,
        'indices': indices,
        'stats': stats
    }
```

### Index Optimization and Management
```python
def optimize_indices(target_indices=None):
    """
    Perform optimization operations on indices.
    
    Parameters:
    - target_indices: List of indices to optimize, or None for all
    """
    # Get all indices if not specified
    if not target_indices:
        indices_info = es.cat.indices(format="json")
        target_indices = [idx["index"] for idx in indices_info 
                         if not idx["index"].startswith(".")]
    
    results = {}
    
    for index in target_indices:
        print(f"Optimizing index: {index}")
        
        # Force merge to optimize segments
        merge_result = es.indices.forcemerge(
            index=index,
            max_num_segments=1,  # Consolidate to one segment per shard
            only_expunge_deletes=False
        )
        
        # Refresh the index
        refresh_result = es.indices.refresh(index=index)
        
        # Clear the cache
        cache_result = es.indices.clear_cache(index=index)
        
        results[index] = {
            "forcemerge": merge_result,
            "refresh": refresh_result,
            "clear_cache": cache_result
        }
        
        print(f"Optimization complete for {index}")
    
    return results
```

### Reindexing for Schema Changes
```python
def reindex_with_new_schema(source_index, target_index, mapping):
    """
    Reindex data from a source index to a target index with a new mapping.
    
    Parameters:
    - source_index: The source index name
    - target_index: The target index name (will be created)
    - mapping: The new mapping for the target index
    """
    # Check if source index exists
    if not es.indices.exists(index=source_index):
        raise ValueError(f"Source index {source_index} does not exist")
    
    # Create target index with new mapping
    if es.indices.exists(index=target_index):
        print(f"Target index {target_index} already exists, deleting...")
        es.indices.delete(index=target_index)
    
    print(f"Creating target index {target_index} with new mapping")
    es.indices.create(index=target_index, body=mapping)
    
    # Start reindexing
    print(f"Starting reindex from {source_index} to {target_index}")
    reindex_body = {
        "source": {
            "index": source_index
        },
        "dest": {
            "index": target_index
        }
    }
    
    # Execute reindex operation
    result = es.reindex(body=reindex_body, wait_for_completion=False)
    task_id = result["task"]
    
    print(f"Reindexing started with task ID: {task_id}")
    print("You can check task status with: GET _tasks/{task_id}")
    
    return {
        "task_id": task_id,
        "source_index": source_index,
        "target_index": target_index
    }
```

## Performance Monitoring and Troubleshooting

### Query Performance Analysis
```python
def analyze_slow_queries():
    """
    Analyze slow queries from the Elasticsearch slow log.
    """
    # Get slow search queries
    slow_searches = es.indices.get_settings(
        index="_all",
        name="index.search.slowlog.threshold*"
    )
    
    print("Current Slow Log Settings:")
    for index, settings in slow_searches.items():
        if "settings" in settings and "index" in settings["settings"]:
            idx_settings = settings["settings"]["index"]
            if "search" in idx_settings and "slowlog" in idx_settings["search"]:
                slowlog = idx_settings["search"]["slowlog"]
                print(f"\nIndex: {index}")
                for key, value in slowlog.items():
                    if key.startswith("threshold"):
                        print(f"  {key}: {value}")
    
    # Get recent slow logs
    try:
        slow_logs = es.search(
            index=".monitoring-es-*",
            body={
                "query": {
                    "bool": {
                        "must": [
                            {"term": {"type": "index_search_slowlog"}},
                            {"range": {"timestamp": {"gte": "now-7d"}}}
                        ]
                    }
                },
                "sort": [{"timestamp": {"order": "desc"}}],
                "size": 100
            }
        )
        
        print("\nRecent Slow Queries:")
        for hit in slow_logs["hits"]["hits"]:
            source = hit["_source"]
            print(f"\nTimestamp: {source.get('timestamp')}")
            print(f"Index: {source.get('index_name')}")
            print(f"Took: {source.get('took_millis')}ms")
            print(f"Query: {source.get('query')}")
    except Exception as e:
        print(f"Error fetching slow logs: {str(e)}")
        print("Note: You may need to enable slow logs and monitoring.")
    
    # Provide recommendations
    print("\nRecommendations for Optimizing Slow Queries:")
    print("1. Use filter context instead of query context when possible")
    print("2. Limit the number of fields in your queries")
    print("3. Use more specific query types instead of wildcard queries")
    print("4. Consider adding appropriate field mappings and analyzers")
    print("5. Review shard allocation and sizing")
```

### Cluster Monitoring
```python
def monitor_cluster_health():
    """
    Comprehensive monitoring of Elasticsearch cluster health.
    """
    # Get cluster health
    health = es.cluster.health()
    
    print("Cluster Health:")
    print(f"Status: {health['status']}")
    print(f"Nodes: {health['number_of_nodes']}")
    print(f"Data Nodes: {health['number_of_data_nodes']}")
    print(f"Active Shards: {health['active_shards']}")
    print(f"Relocating Shards: {health['relocating_shards']}")
    print(f"Initializing Shards: {health['initializing_shards']}")
    print(f"Unassigned Shards: {health['unassigned_shards']}")
    
    # Get node stats
    node_stats = es.nodes.stats()
    
    print("\nNode Stats:")
    for node_id, stats in node_stats["nodes"].items():
        print(f"\nNode: {stats.get('name', node_id)}")
        print(f"CPU: {stats.get('os', {}).get('cpu', {}).get('percent', 'N/A')}%")
        print(f"Load: {stats.get('os', {}).get('load_average', {}).get('1m', 'N/A')}")
        
        # Memory usage
        if "jvm" in stats:
            heap_used = stats["jvm"]["mem"]["heap_used_in_bytes"]
            heap_max = stats["jvm"]["mem"]["heap_max_in_bytes"]
            heap_percent = (heap_used / heap_max) * 100 if heap_max > 0 else 0
            print(f"JVM Heap: {heap_percent:.1f}% ({heap_used/(1024*1024):.1f}MB / {heap_max/(1024*1024):.1f}MB)")
        
        # Indices stats
        if "indices" in stats:
            docs = stats["indices"]["docs"]["count"]
            size = stats["indices"].get("store", {}).get("size_in_bytes", 0)
            print(f"Documents: {docs}")
            print(f"Size: {size/(1024*1024*1024):.2f}GB")
    
    # Get pending tasks
    pending = es.cluster.pending_tasks()
    
    print("\nPending Tasks:")
    if pending["tasks"]:
        for task in pending["tasks"]:
            print(f"Priority: {task['priority']}")
            print(f"Source: {task['source']}")
            print(f"Time in Queue: {task['time_in_queue_millis']}ms")
            print(f"Action: {task['action']}")
    else:
        print("No pending tasks")
    
    return {
        "health": health,
        "node_stats": node_stats,
        "pending_tasks": pending
    }
```

## Common Product Matching Patterns

### Fuzzy Matching with Multiple Techniques
```python
from rapidfuzz import fuzz
import re

def match_product_with_fuzzy_logic(distributor_product, catalogue_products, 
                                    min_score=0.6, mrp_tolerance=0.1):
    """
    Match a distributor product with catalogue products using multiple fuzzy matching techniques.
    
    Parameters:
    - distributor_product: Dictionary with distributor product details
    - catalogue_products: List of catalogue product dictionaries
    - min_score: Minimum score threshold for matches (0-1)
    - mrp_tolerance: MRP percentage tolerance for matching
    
    Returns a list of matches sorted by score.
    """
    # Extract distributor product details
    dist_name = distributor_product.get('product_name', '')
    dist_normalized = normalize_product_name(dist_name)
    dist_manufacturer = distributor_product.get('manufacturer', '')
    dist_mrp = float(distributor_product.get('mrp', 0))
    dist_package = distributor_product.get('package', '')
    
    # Prepare results list
    matches = []
    
    for catalogue_item in catalogue_products:
        # Extract catalogue product details
        cat_name = catalogue_item.get('name', '')
        cat_normalized = normalize_product_name(cat_name)
        cat_manufacturer = catalogue_item.get('manufacturers', '')
        cat_mrp = float(catalogue_item.get('mrp', 0))
        cat_package = catalogue_item.get('package_detail', '')
        
        # Skip if names are too different in length
        len_diff = abs(len(dist_normalized) - len(cat_normalized))
        if len_diff > 10 and (len_diff / max(len(dist_normalized), len(cat_normalized))) > 0.5:
            continue
        
        # Skip if MRP is too different (when both are present)
        if dist_mrp > 0 and cat_mrp > 0:
            mrp_diff = abs(dist_mrp - cat_mrp)
            relative_diff = mrp_diff / max(dist_mrp, cat_mrp)
            if relative_diff > mrp_tolerance:
                continue
        
        # Calculate multiple similarity scores
        token_sort_score = fuzz.token_sort_ratio(dist_normalized, cat_normalized) / 100
        token_set_score = fuzz.token_set_ratio(dist_normalized, cat_normalized) / 100
        partial_ratio = fuzz.partial_ratio(dist_normalized, cat_normalized) / 100
        
        # Substring bonus
        substring_bonus = 0.2 if (dist_normalized in cat_normalized or 
                               cat_normalized in dist_normalized) else 0
        
        # Calculate name similarity (50% weight)
        name_score = (token_sort_score * 0.3 + token_set_score * 0.4 + 
                     partial_ratio * 0.3 + substring_bonus)
        
        # Calculate MRP similarity (20% weight)
        if dist_mrp > 0 and cat_mrp > 0:
            mrp_score = max(0, 1 - min(relative_diff, 1))
        else:
            mrp_score = 0.5  # Neutral score if either MRP is missing
        
        # Calculate manufacturer similarity (15% weight)
        manufacturer_score = fuzz.token_set_ratio(dist_manufacturer, cat_manufacturer) / 100
        
        # Calculate package similarity (15% weight)
        package_score = fuzz.token_set_ratio(dist_package, cat_package) / 100
        
        # Calculate final weighted score
        final_score = (name_score * 0.5 + 
                      mrp_score * 0.2 + 
                      manufacturer_score * 0.15 + 
                      package_score * 0.15)
        
        # Add to matches if above threshold
        if final_score >= min_score:
            matches.append({
                'catalogue_id': catalogue_item.get('product_id', ''),
                'catalogue_name': cat_name,
                'score': final_score,
                'name_score': name_score,
                'mrp_score': mrp_score,
                'manufacturer_score': manufacturer_score,
                'package_score': package_score
            })
    
    # Sort matches by score (descending)
    matches.sort(key=lambda x: x['score'], reverse=True)
    
    return matches
```

### Product Matching with Confidence Levels
```python
def match_with_confidence_levels(distributor_product, catalogue_products):
    """
    Match products with confidence level classification.
    
    Parameters:
    - distributor_product: Dictionary with distributor product details
    - catalogue_products: List of catalogue product dictionaries
    
    Returns matches with confidence levels.
    """
    # Get fuzzy matches
    all_matches = match_product_with_fuzzy_logic(
        distributor_product, 
        catalogue_products,
        min_score=0.3  # Lower threshold to get more candidates
    )
    
    # Classify matches by confidence level
    high_confidence = []
    medium_confidence = []
    low_confidence = []
    
    for match in all_matches:
        score = match['score']
        name_score = match['name_score']
        mrp_score = match['mrp_score']
        
        # Determine confidence level
        if score >= 0.9 or (score >= 0.8 and name_score >= 0.9 and mrp_score >= 0.9):
            high_confidence.append(match)
        elif score >= 0.7 or (score >= 0.6 and name_score >= 0.7 and mrp_score >= 0.8):
            medium_confidence.append(match)
        else:
            low_confidence.append(match)
    
    return {
        'high_confidence': high_confidence,
        'medium_confidence': medium_confidence,
        'low_confidence': low_confidence,
        'best_match': all_matches[0] if all_matches else None
    }
```

## Query Examples for Product Search

### Basic Product Search by Name
```python
def search_products_by_name(query_text, size=20):
    """Basic product search by name"""
    query = {
        "query": {
            "match": {
                "name": {
                    "query": query_text,
                    "fuzziness": "AUTO"
                }
            }
        },
        "size": size
    }
    
    return es.search(index="plazza-catalogue", body=query)
```

### Advanced Product Search with Filters
```python
def search_products_advanced(query_text, manufacturers=None, min_price=None, 
                           max_price=None, prescription_required=None, size=20):
    """Advanced product search with filters"""
    # Build must clauses
    must_clauses = [
        {
            "match": {
                "name": {
                    "query": query_text,
                    "fuzziness": "AUTO"
                }
            }
        }
    ]
    
    # Build filter clauses
    filter_clauses = []
    
    if manufacturers:
        if isinstance(manufacturers, list):
            filter_clauses.append({
                "terms": {"manufacturers.keyword": manufacturers}
            })
        else:
            filter_clauses.append({
                "term": {"manufacturers.keyword": manufacturers}
            })
    
    if min_price is not None or max_price is not None:
        range_filter = {"range": {"mrp": {}}}
        if min_price is not None:
            range_filter["range"]["mrp"]["gte"] = min_price
        if max_price is not None:
            range_filter["range"]["mrp"]["lte"] = max_price
        filter_clauses.append(range_filter)
    
    if prescription_required is not None:
        filter_clauses.append({
            "term": {"prescription_required": prescription_required}
        })
    
    # Build complete query
    query = {
        "query": {
            "bool": {
                "must": must_clauses
            }
        },
        "size": size
    }
    
    if filter_clauses:
        query["query"]["bool"]["filter"] = filter_clauses
    
    return es.search(index="plazza-catalogue", body=query)
```

### Product Autocompletion
```python
def get_product_autocompletion(prefix, size=10):
    """Get product name autocompletion suggestions"""
    query = {
        "suggest": {
            "product_suggestions": {
                "prefix": prefix,
                "completion": {
                    "field": "product_suggest",
                    "size": size,
                    "skip_duplicates": True,
                    "fuzzy": {
                        "fuzziness": 1
                    }
                }
            }
        },
        "_source": ["product_id", "name", "manufacturers", "mrp"]
    }
    
    return es.search(index="plazza-catalogue", body=query)
```

### Multi-Field Product Search
```python
def multi_field_product_search(query_text, size=20):
    """Search across multiple fields with different boosts"""
    query = {
        "query": {
            "bool": {
                "should": [
                    {
                        "match": {
                            "name": {
                                "query": query_text,
                                "boost": 3.0
                            }
                        }
                    },
                    {
                        "match": {
                            "manufacturers": {
                                "query": query_text,
                                "boost": 1.5
                            }
                        }
                    },
                    {
                        "match": {
                            "salt_composition": {
                                "query": query_text,
                                "boost": 1.0
                            }
                        }
                    }
                ]
            }
        },
        "size": size
    }
    
    return es.search(index="plazza-catalogue", body=query)
```

## Elasticsearch Index and Mapping Management

### Get Index Mapping
```python
def get_index_mapping(index_name):
    """Get the mapping of an index"""
    return es.indices.get_mapping(index=index_name)
```

### Add Field to Existing Index
```python
def add_field_to_index(index_name, field_name, field_mapping):
    """Add a new field to an existing index"""
    # Create the mapping update
    mapping_update = {
        "properties": {
            field_name: field_mapping
        }
    }
    
    # Update the mapping
    result = es.indices.put_mapping(
        index=index_name,
        body=mapping_update
    )
    
    return result
```

### Add Alias to Index
```python
def add_index_alias(index_name, alias_name):
    """Add an alias to an index"""
    result = es.indices.put_alias(
        index=index_name,
        name=alias_name
    )
    
    return result
```

### Create Index Template
```python
def create_index_template(template_name, pattern, mappings, settings):
    """Create an index template"""
    template_body = {
        "index_patterns": [pattern],
        "mappings": mappings,
        "settings": settings
    }
    
    result = es.indices.put_template(
        name=template_name,
        body=template_body
    )
    
    return result
```

## Managing Elasticsearch Security

### Create API Key
```python
def create_api_key(name, role_descriptors=None, expiration="7d"):
    """Create an API key with specified roles"""
    body = {
        "name": name,
        "expiration": expiration
    }
    
    if role_descriptors:
        body["role_descriptors"] = role_descriptors
    
    result = es.security.create_api_key(body=body)
    
    # Encode the key
    api_key = base64.b64encode(f"{result['id']}:{result['api_key']}".encode()).decode()
    
    print(f"API Key created: {result['name']} (ID: {result['id']})")
    print(f"Encoded API Key: {api_key}")
    print(f"Expiration: {result['expiration']}")
    
    return {
        "id": result["id"],
        "encoded_api_key": api_key,
        "expiration": result["expiration"]
    }
```

### Check API Key Info
```python
def check_api_key_info(api_key_id):
    """Get information about an API key"""
    try:
        result = es.security.get_api_key(id=api_key_id)
        return result
    except Exception as e:
        print(f"Error checking API key: {str(e)}")
        return None
```

### Invalidate API Key
```python
def invalidate_api_key(api_key_id):
    """Invalidate an API key"""
    try:
        result = es.security.invalidate_api_key(
            body={"id": api_key_id}
        )
        return result
    except Exception as e:
        print(f"Error invalidating API key: {str(e)}")
        return None
```