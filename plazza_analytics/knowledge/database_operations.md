# Database Operations Knowledge Base - 2025-03-26

## Metadata
- **Documentation Date**: 2025-03-26
- **Documentation Type**: Technical
- **Data Domains**: Database Operations, Query Patterns, Synchronization
- **Generated By**: Claude Code

## Database Connection Best Practices

### Connection Pooling with ThreadedConnectionPool
```python
from psycopg2.pool import ThreadedConnectionPool
import os

# Setup connection pool for efficient connection management
pool = ThreadedConnectionPool(
    minconn=2,  # Minimum connections to maintain
    maxconn=10, # Maximum connections allowed
    dsn=os.getenv('DATABASE_URL')
)

# Context manager for proper connection handling
@contextmanager
def get_db_connection():
    conn = None
    try:
        conn = pool.getconn()
        yield conn
    finally:
        if conn:
            pool.putconn(conn)
```

### Context Managers for Cursor Management
```python
@contextmanager
def get_db_cursor(cursor_factory=psycopg2.extras.RealDictCursor, commit=True):
    with get_db_connection() as conn:
        cursor = conn.cursor(cursor_factory=cursor_factory)
        try:
            yield cursor
            if commit:
                conn.commit()
        except Exception as e:
            conn.rollback()
            raise
        finally:
            cursor.close()
```

## Efficient Query Patterns

### Parameterized Queries
```python
# ALWAYS use parameterized queries to avoid SQL injection
with get_db_cursor() as cursor:
    cursor.execute(
        "SELECT * FROM all_products WHERE product_name LIKE %s AND price < %s",
        (f"%{search_term}%", max_price)
    )
    results = cursor.fetchall()
```

### Batch Processing for Large Datasets
```python
from psycopg2.extras import execute_batch

# Process records in batches for better performance
batch_data = []
for record in records:
    batch_data.append((
        record['product_name'],
        record['manufacturer'],
        record['mrp']
    ))

# Use execute_batch for efficient batch inserts
sql = "INSERT INTO products (product_name, manufacturer, mrp) VALUES (%s, %s, %s)"
execute_batch(cursor, sql, batch_data, page_size=1000)
```

### Upsert Pattern (INSERT ON CONFLICT)
```python
# Update if exists, insert if not
sql = """
INSERT INTO distributor_master_list 
    (product_name, manufacturer, mrp, purchase_rate, distributor) 
VALUES 
    (%s, %s, %s, %s, %s)
ON CONFLICT (item_code) 
DO UPDATE SET 
    product_name = EXCLUDED.product_name,
    manufacturer = EXCLUDED.manufacturer,
    mrp = EXCLUDED.mrp,
    purchase_rate = EXCLUDED.purchase_rate,
    updated_at = CURRENT_TIMESTAMP
"""
cursor.execute(sql, (product_name, manufacturer, mrp, purchase_rate, distributor))
```

## Schema Verification Queries

### Get Column Types and Properties
```python
def get_table_schema(table_name):
    with get_db_cursor() as cursor:
        cursor.execute("""
            SELECT 
                column_name, 
                data_type, 
                is_nullable,
                column_default
            FROM 
                information_schema.columns 
            WHERE 
                table_name = %s
                AND table_schema = 'public'
            ORDER BY 
                ordinal_position
        """, (table_name,))
        return cursor.fetchall()
```

### Count Records in Tables
```python
def get_table_counts():
    with get_db_cursor() as cursor:
        cursor.execute("""
            SELECT 
                table_name, 
                n_live_tup as record_count
            FROM 
                pg_stat_user_tables
            ORDER BY 
                n_live_tup DESC
        """)
        return cursor.fetchall()
```

### Check for Database Constraints
```python
def get_table_constraints(table_name):
    with get_db_cursor() as cursor:
        cursor.execute("""
            SELECT 
                c.conname as constraint_name,
                c.contype as constraint_type,
                pg_get_constraintdef(c.oid) as constraint_definition
            FROM 
                pg_constraint c
                JOIN pg_namespace n ON n.oid = c.connamespace
                JOIN pg_class cl ON cl.oid = c.conrelid
            WHERE 
                cl.relname = %s
                AND n.nspname = 'public'
        """, (table_name,))
        return cursor.fetchall()
```

## Elasticsearch Synchronization Techniques

### Bulk Indexing
```python
from elasticsearch import Elasticsearch, helpers
import os

# Connect to Elasticsearch
es = Elasticsearch(
    os.getenv('ES_HOST'),
    basic_auth=(os.getenv('ES_USER'), os.getenv('ES_PASSWORD')),
    verify_certs=False
)

# Prepare actions for bulk indexing
actions = []
for record in records:
    action = {
        '_index': 'distributor-master-list',
        '_id': record['id'],
        '_source': {
            'product_name': record['product_name'],
            'manufacturer': record['manufacturer'],
            'normalized_name': normalize_product_name(record['product_name']),
            'item_code': record['item_code'],
            'mrp': float(record['mrp']),
            'updated_at': datetime.now().isoformat()
        }
    }
    actions.append(action)

# Execute bulk indexing
success, errors = helpers.bulk(es, actions, stats_only=True)
print(f"Indexed {success} documents with {len(errors)} errors")
```

### Partial Document Updates
```python
# Update specific fields without overwriting the whole document
update_body = {
    "doc": {
        "product_id": matched_product_id,
        "updated_at": datetime.now().isoformat()
    }
}
es.update(index="distributor-master-list", id=document_id, body=update_body)
```

### Efficient Elasticsearch Queries
```python
# Build a complex query with multiple matching approaches
def build_catalogue_query(product_name, mrp=None, manufacturer=None):
    normalized_name = normalize_product_name(product_name)
    
    should_clauses = [
        {"match": {"name": {"query": normalized_name, "fuzziness": "AUTO"}}},
        {"match_phrase_prefix": {"name": normalized_name}},
        {"wildcard": {"normalized_name": f"*{normalized_name}*"}},
    ]
    
    # Add individual token matching for better partial matches
    tokens = normalized_name.split()
    for token in tokens:
        if len(token) > 3:  # Only use tokens with significant length
            should_clauses.append({"match": {"name": {"query": token, "boost": 0.5}}})
    
    query = {
        "query": {
            "bool": {
                "should": should_clauses,
                "minimum_should_match": 1
            }
        },
        "size": 50  # Return top 50 matches
    }
    
    # Add price filter if MRP is provided
    if mrp and mrp > 0:
        query["query"]["bool"]["filter"] = [{
            "range": {
                "mrp": {
                    "gte": mrp * 0.9,  # Allow 10% lower
                    "lte": mrp * 1.1   # Allow 10% higher
                }
            }
        }]
    
    # Add manufacturer filter if provided
    if manufacturer:
        query["query"]["bool"]["should"].append({
            "match": {
                "manufacturer": {
                    "query": manufacturer,
                    "boost": 2.0  # Give higher importance to manufacturer match
                }
            }
        })
    
    return query
```

## Cross-System UUID Mapping Strategies

### Deterministic UUID Generation
```python
import uuid

# Generate consistent UUIDs from string identifiers
def generate_deterministic_uuid(string_id):
    """
    Generate a deterministic UUID based on a string identifier.
    This ensures the same string always produces the same UUID.
    """
    # Use NAMESPACE_DNS for consistency
    return str(uuid.uuid5(uuid.NAMESPACE_DNS, string_id))

# Example: Generate UUIDs for contact IDs
contact_uuid = generate_deterministic_uuid(contact_id)
```

### ID Mapping Table Management
```python
def get_id_mapping(table_name, external_id_field, db_id_field='id'):
    """
    Create a mapping between external IDs and database UUIDs.
    """
    mapping = {}
    
    with get_db_cursor() as cursor:
        cursor.execute(f"""
            SELECT {db_id_field}, {external_id_field}
            FROM {table_name}
            WHERE {external_id_field} IS NOT NULL
        """)
        
        for row in cursor.fetchall():
            # Map external ID to database UUID
            external_id = row[external_id_field]
            db_id = row[db_id_field]
            
            if external_id:
                mapping[external_id] = db_id
    
    return mapping
```

### ID Resolution for Cross-System Sync
```python
# When syncing data between systems, use mappings for ID resolution
def resolve_foreign_key(record, field, mapping, fallback_method='deterministic'):
    """
    Resolve foreign key references using ID mappings with fallback options.
    """
    if field not in record:
        return None
        
    field_value = record[field]
    
    # First try direct mapping
    if field_value in mapping:
        return mapping[field_value]
        
    # Fallback to deterministic UUID if specified
    if fallback_method == 'deterministic':
        return generate_deterministic_uuid(str(field_value))
        
    # Default to None if not found
    return None
```

## Incremental Data Sync with Timestamp Tracking

### Sync Control Table Management
```python
# Create a sync_control table for tracking synchronization state
def create_sync_control_table():
    with get_db_cursor() as cursor:
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS sync_control (
                source_table VARCHAR(255) PRIMARY KEY,
                last_sync_time TIMESTAMP NOT NULL,
                records_processed INTEGER NOT NULL DEFAULT 0,
                created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
            )
        """)

# Get last sync time for a table
def get_last_sync_time(table_name):
    with get_db_cursor() as cursor:
        cursor.execute("""
            SELECT last_sync_time
            FROM sync_control
            WHERE source_table = %s
        """, (table_name,))
        
        result = cursor.fetchone()
        if result:
            return result['last_sync_time']
        else:
            # Default to a timestamp in the past if no sync record exists
            default_time = datetime.now() - timedelta(days=365)  # 1 year ago
            return default_time

# Update sync control after successful sync
def update_sync_control(table_name, records_processed):
    now = datetime.now()
    
    with get_db_cursor() as cursor:
        cursor.execute("""
            INSERT INTO sync_control 
                (source_table, last_sync_time, records_processed, updated_at)
            VALUES 
                (%s, %s, %s, %s)
            ON CONFLICT (source_table) 
            DO UPDATE SET 
                last_sync_time = EXCLUDED.last_sync_time,
                records_processed = sync_control.records_processed + EXCLUDED.records_processed,
                updated_at = EXCLUDED.updated_at
        """, (table_name, now, records_processed, now))
```

### Timestamp-Based Incremental Updates
```python
# Example: Sync updated products from one system to another
def sync_updated_products():
    # Get last sync time
    last_sync = get_last_sync_time('products')
    
    # Format for query
    last_sync_str = last_sync.strftime('%Y-%m-%d %H:%M:%S')
    
    # Fetch updated records
    with get_db_cursor() as cursor:
        cursor.execute("""
            SELECT *
            FROM products
            WHERE updated_at > %s
            ORDER BY updated_at
        """, (last_sync_str,))
        
        updated_records = cursor.fetchall()
    
    # Process the updated records
    processed_count = 0
    for record in updated_records:
        # Sync logic here...
        processed_count += 1
    
    # Update sync control
    if processed_count > 0:
        update_sync_control('products', processed_count)
        
    return processed_count
```

## Database Health Monitoring

### Critical Health Metrics Queries
```python
# Check table sizes
def get_table_sizes():
    with get_db_cursor() as cursor:
        cursor.execute("""
            SELECT 
                tablename AS table_name,
                pg_size_pretty(pg_total_relation_size(quote_ident(tablename))) AS total_size,
                pg_size_pretty(pg_relation_size(quote_ident(tablename))) AS table_size,
                pg_size_pretty(pg_total_relation_size(quote_ident(tablename)) - 
                               pg_relation_size(quote_ident(tablename))) AS index_size
            FROM 
                pg_tables
            WHERE 
                schemaname = 'public'
            ORDER BY 
                pg_total_relation_size(quote_ident(tablename)) DESC
        """)
        return cursor.fetchall()

# Check for slow queries
def get_slow_queries():
    with get_db_cursor() as cursor:
        cursor.execute("""
            SELECT 
                query,
                calls,
                total_exec_time / calls as avg_time,
                min_exec_time,
                max_exec_time,
                mean_exec_time,
                stddev_exec_time,
                rows / calls as avg_rows
            FROM 
                pg_stat_statements
            ORDER BY 
                avg_time DESC
            LIMIT 20
        """)
        return cursor.fetchall()

# Check for table bloat
def get_table_bloat():
    with get_db_cursor() as cursor:
        cursor.execute("""
            SELECT
                schemaname,
                tablename,
                cc.reltuples AS row_estimate,
                pg_size_pretty(cc.relpages::bigint*8*1024) AS table_size,
                pg_size_pretty(cc.relpages::bigint*8*1024 - 
                              CASE WHEN cc.relhasindex THEN pi.pages ELSE 0 END * 8 * 1024) AS bloat_size,
                CASE WHEN cc.reltuples > 0
                     THEN round(100 * (cc.relpages::bigint*8*1024 - 
                              CASE WHEN cc.relhasindex THEN pi.pages ELSE 0 END * 8 * 1024) / 
                              (cc.relpages::bigint*8*1024), 2)
                     ELSE 0
                END AS bloat_percentage
            FROM (
                SELECT 
                    ns.nspname AS schemaname,
                    tbl.relname AS tablename,
                    tbl.reltuples,
                    tbl.relpages,
                    tbl.relhasindex
                FROM 
                    pg_class tbl
                JOIN 
                    pg_namespace ns ON ns.oid = tbl.relnamespace
                WHERE 
                    tbl.relkind = 'r'
                    AND ns.nspname = 'public'
            ) AS cc
            LEFT JOIN (
                SELECT 
                    indrelid,
                    SUM(pages) AS pages
                FROM (
                    SELECT 
                        indexrelid::regclass AS index,
                        s.relid AS indrelid,
                        s.relpages AS pages
                    FROM 
                        pg_index i
                    JOIN 
                        pg_class s ON s.oid = i.indexrelid
                ) AS x
                GROUP BY indrelid
            ) AS pi ON pi.indrelid = cc.tablename::regclass
            ORDER BY bloat_percentage DESC
        """)
        return cursor.fetchall()
```

## Key Database Optimization Techniques

### Creating Effective Indexes
```python
# Create indexes for frequently queried fields
def create_recommended_indexes():
    with get_db_cursor() as cursor:
        # Basic indexes for common lookup fields
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_products_name ON all_products (product_name);
            CREATE INDEX IF NOT EXISTS idx_products_manufacturer ON all_products (manufacturer);
            CREATE INDEX IF NOT EXISTS idx_inventory_product_id ON inventory_products (product_id);
            
            -- Compound index for common query combinations
            CREATE INDEX IF NOT EXISTS idx_inventory_product_expiry ON inventory_products (product_id, expiry_date);
            
            -- Index for timestamp-based queries (like incremental sync)
            CREATE INDEX IF NOT EXISTS idx_products_updated_at ON all_products (updated_at);
            
            -- Full-text search index for product search
            CREATE INDEX IF NOT EXISTS idx_products_name_gin ON all_products USING gin(to_tsvector('english', product_name));
            
            -- Trigram index for fuzzy matching on product names
            CREATE EXTENSION IF NOT EXISTS pg_trgm;
            CREATE INDEX IF NOT EXISTS idx_products_name_trgm ON all_products USING gin (product_name gin_trgm_ops);
        """)
```

### Table Partitioning for Large Tables
```python
# Create a partitioned table for orders by date
def create_partitioned_orders_table():
    with get_db_cursor() as cursor:
        # Create the partitioned table
        cursor.execute("""
            CREATE TABLE orders_partitioned (
                id UUID PRIMARY KEY,
                contact_id UUID NOT NULL,
                order_date DATE NOT NULL,
                status VARCHAR(50) NOT NULL,
                amount NUMERIC(10,2) NOT NULL,
                created_at TIMESTAMP NOT NULL,
                updated_at TIMESTAMP NOT NULL
            ) PARTITION BY RANGE (order_date);
            
            -- Create partitions by month
            CREATE TABLE orders_y2025m01 PARTITION OF orders_partitioned
                FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');
                
            CREATE TABLE orders_y2025m02 PARTITION OF orders_partitioned
                FOR VALUES FROM ('2025-02-01') TO ('2025-03-01');
                
            CREATE TABLE orders_y2025m03 PARTITION OF orders_partitioned
                FOR VALUES FROM ('2025-03-01') TO ('2025-04-01');
                
            -- Add indexes on partitioned table
            CREATE INDEX idx_orders_part_contact_id ON orders_partitioned (contact_id);
            CREATE INDEX idx_orders_part_status ON orders_partitioned (status);
        """)
```

### Database Maintenance Tasks
```python
# Run routine database maintenance tasks
def perform_database_maintenance():
    with get_db_cursor() as cursor:
        # Analyze tables to update statistics
        cursor.execute("ANALYZE VERBOSE")
        
        # Vacuum tables to reclaim space and update statistics
        cursor.execute("VACUUM (VERBOSE, ANALYZE)")
        
        # Reindex tables to rebuild indexes
        cursor.execute("REINDEX DATABASE current_database()")
```

## Common Database Query Examples

### Complex JOIN Queries
```python
# Query to get orders with customer and product details
def get_orders_with_details(start_date, end_date):
    with get_db_cursor() as cursor:
        cursor.execute("""
            SELECT 
                o.order_id,
                o.created_at as order_date,
                o.bill_total_amount,
                o.status,
                c.first_name || ' ' || c.last_name as customer_name,
                c.contact_id,
                oi.product_id,
                oi.medicine_name,
                oi.quantity,
                oi.mrp,
                oi.selling_price,
                (oi.mrp - oi.selling_price) as discount_amount,
                CASE WHEN oi.mrp > 0 
                    THEN ROUND(((oi.mrp - oi.selling_price) / oi.mrp) * 100, 2)
                    ELSE 0
                END as discount_percentage
            FROM 
                orders o
            JOIN 
                contacts c ON o.contact_id = c.id
            JOIN 
                order_items oi ON o.order_id = oi.order_id
            WHERE 
                o.created_at BETWEEN %s AND %s
            ORDER BY 
                o.created_at DESC
        """, (start_date, end_date))
        
        return cursor.fetchall()
```

### Window Functions for Analytics
```python
# Get trending products by month
def get_trending_products_by_month():
    with get_db_cursor() as cursor:
        cursor.execute("""
            WITH monthly_sales AS (
                SELECT 
                    oi.product_id,
                    oi.medicine_name,
                    DATE_TRUNC('month', o.created_at) as month,
                    SUM(oi.quantity) as total_quantity,
                    SUM(oi.quantity * oi.selling_price) as total_revenue,
                    COUNT(DISTINCT o.order_id) as order_count
                FROM 
                    order_items oi
                JOIN 
                    orders o ON oi.order_id = o.order_id
                WHERE 
                    o.created_at >= CURRENT_DATE - INTERVAL '6 months'
                GROUP BY 
                    oi.product_id, oi.medicine_name, DATE_TRUNC('month', o.created_at)
            ),
            ranked_products AS (
                SELECT 
                    product_id,
                    medicine_name,
                    month,
                    total_quantity,
                    total_revenue,
                    order_count,
                    ROW_NUMBER() OVER (PARTITION BY month ORDER BY total_quantity DESC) as quantity_rank,
                    ROW_NUMBER() OVER (PARTITION BY month ORDER BY total_revenue DESC) as revenue_rank
                FROM 
                    monthly_sales
            )
            SELECT 
                product_id,
                medicine_name,
                month,
                total_quantity,
                total_revenue,
                order_count,
                quantity_rank,
                revenue_rank
            FROM 
                ranked_products
            WHERE 
                quantity_rank <= 10 OR revenue_rank <= 10
            ORDER BY 
                month DESC, revenue_rank
        """)
        
        return cursor.fetchall()
```

### Common Reporting Queries
```python
# Get daily sales summary for dashboard
def get_daily_sales_summary(days=30):
    with get_db_cursor() as cursor:
        cursor.execute("""
            SELECT 
                DATE(created_at) as order_date,
                COUNT(*) as order_count,
                SUM(bill_total_amount) as total_revenue,
                AVG(bill_total_amount) as average_order_value,
                COUNT(DISTINCT contact_id) as customer_count
            FROM 
                orders
            WHERE 
                created_at >= CURRENT_DATE - INTERVAL %s DAY
                AND status NOT IN ('cancelled', 'returned')
            GROUP BY 
                DATE(created_at)
            ORDER BY 
                order_date DESC
        """, (days,))
        
        return cursor.fetchall()
```

## Database Security Best Practices

### Securing Database Connections
```python
# Secure connection string handling
def get_secure_connection_string():
    # Never hard-code credentials
    host = os.getenv('DB_HOST')
    port = os.getenv('DB_PORT', '26257')  # CockroachDB default port
    user = os.getenv('DB_USER')
    password = os.getenv('DB_PASSWORD')
    dbname = os.getenv('DB_NAME')
    sslmode = os.getenv('DB_SSL_MODE', 'verify-full')
    sslrootcert = os.getenv('DB_SSL_ROOT_CERT', '/path/to/ca.crt')
    
    # Build connection string with all security parameters
    conn_string = f"postgresql://{user}:{password}@{host}:{port}/{dbname}"
    conn_string += f"?sslmode={sslmode}&sslrootcert={sslrootcert}"
    
    return conn_string
```

### Database User Management
```python
# Create users with appropriate permissions
def create_database_users():
    # Connect as admin
    with get_admin_connection() as conn:
        with conn.cursor() as cursor:
            # Create application user with limited permissions
            cursor.execute("""
                CREATE USER app_user WITH PASSWORD 'secure_password';
                GRANT CONNECT ON DATABASE defaultdb TO app_user;
                GRANT USAGE ON SCHEMA public TO app_user;
                
                -- Grant read-only access to specific tables
                GRANT SELECT ON TABLE all_products, inventory_products TO app_user;
                
                -- Grant read-write access to specific tables
                GRANT SELECT, INSERT, UPDATE ON TABLE orders, order_items TO app_user;
                
                -- Deny delete permissions
                REVOKE DELETE ON ALL TABLES IN SCHEMA public FROM app_user;
                
                -- Create reporting user with read-only access
                CREATE USER report_user WITH PASSWORD 'report_password';
                GRANT CONNECT ON DATABASE defaultdb TO report_user;
                GRANT USAGE ON SCHEMA public TO report_user;
                GRANT SELECT ON ALL TABLES IN SCHEMA public TO report_user;
            """)
```

This comprehensive knowledge base provides detailed patterns, examples, and best practices for database operations in the Plazza ecosystem, tailored for your multi-agent system.