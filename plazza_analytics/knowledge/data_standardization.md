# Data Standardization and Product Matching Knowledge Base - 2025-03-26

## Metadata
- **Documentation Date**: 2025-03-26
- **Documentation Type**: Technical
- **Data Domains**: Data Standardization, Product Matching, Fuzzy Matching
- **Generated By**: Claude Code

## Data Standardization Framework

### Base Standardizer Class
```python
import pandas as pd
import re
import os
import logging
from datetime import datetime
from decimal import Decimal, InvalidOperation, ROUND_HALF_UP

class DataStandardizer:
    """
    Base class for standardizing data from various sources.
    
    This class provides common functionality for standardizing different
    types of data into a consistent format.
    """
    
    def __init__(self, name, input_file, output_dir):
        """
        Initialize the standardizer.
        
        Args:
            name (str): Name of the standardizer
            input_file (str): Path to the input file
            output_dir (str): Directory for output files
        """
        self.name = name
        self.input_file = input_file
        self.output_dir = output_dir
        
        # Setup logging
        self.logger = logging.getLogger(name)
        self.logger.setLevel(logging.INFO)
        
        # Ensure output directory exists
        os.makedirs(output_dir, exist_ok=True)
        
        # Setup file handler for logging
        log_file = os.path.join(output_dir, f"{self.name}.log")
        handler = logging.FileHandler(log_file)
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
        
        # Also log to console
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        self.logger.addHandler(console_handler)
    
    def load_data(self):
        """
        Load data from the input file.
        Override in subclasses for specific file types.
        """
        raise NotImplementedError("Subclasses must implement load_data()")
    
    def standardize(self, data):
        """
        Standardize the loaded data.
        Override in subclasses for specific standardization logic.
        
        Args:
            data: The data to standardize
            
        Returns:
            Standardized data
        """
        raise NotImplementedError("Subclasses must implement standardize()")
    
    def save_output(self, standardized_data, error_data=None):
        """
        Save the standardized data to the output directory.
        
        Args:
            standardized_data: The standardized data to save
            error_data: Optional data that failed standardization
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save standardized data
        output_file = os.path.join(self.output_dir, f"standardized_{self.name.lower()}.csv")
        standardized_data.to_csv(output_file, index=False)
        self.logger.info(f"Saved standardized data to {output_file} with {len(standardized_data)} records")
        
        # Save error data if provided
        if error_data is not None and len(error_data) > 0:
            error_file = os.path.join(self.output_dir, f"{self.name}_errors.csv")
            error_data.to_csv(error_file, index=False)
            self.logger.info(f"Saved error data to {error_file} with {len(error_data)} records")
    
    def process(self):
        """
        Process the data through the entire standardization pipeline.
        """
        self.logger.info(f"Starting standardization process for {self.name}")
        
        # Load data
        self.logger.info(f"Loading data from {self.input_file}")
        data = self.load_data()
        
        # Standardize data
        self.logger.info("Standardizing data")
        standardized_data, error_data = self.standardize(data)
        
        # Save results
        self.save_output(standardized_data, error_data)
        
        self.logger.info("Standardization process complete")
        return standardized_data, error_data
    
    def safe_decimal(self, value, decimal_places=2, default=None):
        """
        Safely convert a value to a Decimal with the specified number of decimal places.
        
        Args:
            value: Value to convert
            decimal_places: Number of decimal places to round to
            default: Default value to return if conversion fails
            
        Returns:
            Decimal value or default if conversion fails
        """
        if pd.isna(value):
            return default
        
        try:
            # Convert to string first to handle various input types
            decimal_value = Decimal(str(value))
            
            # Round to specified decimal places
            rounded = decimal_value.quantize(
                Decimal('0.1') ** decimal_places,
                rounding=ROUND_HALF_UP
            )
            
            return rounded
        except (InvalidOperation, ValueError, TypeError):
            return default
    
    def normalize_product_name(self, name):
        """
        Normalize a product name for consistency.
        
        Args:
            name: The product name to normalize
            
        Returns:
            Normalized product name
        """
        if pd.isna(name) or name == "":
            return ""
        
        # Convert to string and strip whitespace
        name = str(name).strip()
        
        # Remove leading/trailing dots and other punctuation
        name = re.sub(r'^[.\s]+|[.\s]+$', '', name)
        
        # Proper case with first letter of each word capitalized
        name = ' '.join(word.capitalize() for word in name.split())
        
        # Handle special cases like "pH" correctly
        name = re.sub(r'\bPh\b', 'pH', name)
        
        return name
```

### Distributor Data Standardizer Example
```python
class MasterDistributorStandardizer(DataStandardizer):
    """
    Standardizer for the master distributor product Excel file.
    
    This class processes the master distributor products file
    and standardizes its structure for import or analysis.
    """
    
    def __init__(self, input_file="data/Copy of All distributor products master.xlsx", 
                 output_dir="standardized_output"):
        """Initialize the Master Distributor Standardizer"""
        super().__init__("MasterDistributor", input_file, output_dir)
        
        # Column mapping from source to standardized format
        self.column_mapping = {
            'item name': 'product_name',
            'company': 'manufacturer',
            'item code': 'item_code',
            'packing': 'package',
            'hsn code': 'hsn_code',
            'gst': 'gst_percentage',
            'mrp ': 'mrp',  # Note the space after 'mrp'
            'salerate': 'purchase_rate',
            'distributer name': 'distributor'
        }
        
        # Fields that must be present in valid records
        self.required_fields = ['product_name', 'mrp', 'package', 'gst_percentage']
        
        # Patterns for invalid product names
        self.placeholder_names = [
            r'^rate diff.*',
            r'^item.*',
            r'^zz.*',
            r'^zzz.*',
            r'^unknown.*',
            r'^misc.*',
            r'^[0-9]+\s+item.*',
            r'^[0-9]+\s+discount.*'
        ]
    
    def load_data(self):
        """Load data from the Excel file"""
        try:
            df = pd.read_excel(self.input_file, engine='openpyxl')
            self.logger.info(f"Loaded {len(df)} rows from {self.input_file}")
            return df
        except Exception as e:
            self.logger.error(f"Error loading data: {str(e)}")
            raise
    
    def standardize_packaging(self, package):
        """Standardize packaging information"""
        if pd.isna(package) or package == "":
            return ""
        
        # Convert to string and strip whitespace
        package = str(package).strip()
        
        # Replace special apostrophes with standard ones
        package = package.replace(''', "'")
        
        # Standardize format for quantities like 10's to 10 units
        package = re.sub(r"(\d+)'s", r"\1 units", package)
        
        # Standardize strip formats like 10x10 to 10x10 Strips
        if re.match(r'^\d+\s*x\s*\d+$', package):
            package += " Strips"
        
        return package
    
    def validate_hsn_code(self, hsn_code):
        """Validate HSN code format"""
        if pd.isna(hsn_code) or hsn_code == "":
            return True, ""
        
        hsn_str = str(hsn_code).strip()
        
        # HSN codes should be 8 digits
        if not re.match(r'^\d{8}$', hsn_str):
            return False, "Invalid HSN code format (should be 8 digits)"
        
        return True, ""
    
    def calculate_price_fields(self, row):
        """
        Calculate Plazza selling price and effective customer discount.
        
        Parameters:
        - row: DataFrame row with product data
        
        Returns:
        - Dictionary with calculated fields
        """
        result = {}
        
        # Default values
        result['plazza_selling_price_incl_gst'] = None
        result['effective_customer_discount'] = None
        
        try:
            # Extract required values
            mrp = self.safe_decimal(row.get('mrp', 0), default=Decimal('0'))
            purchase_rate = self.safe_decimal(row.get('purchase_rate', 0), default=Decimal('0'))
            gst_rate = self.safe_decimal(row.get('gst_percentage', 0), default=Decimal('0')) / 100
            
            # Skip calculation if any required value is missing or zero
            if mrp <= 0 or purchase_rate <= 0 or gst_rate <= 0:
                return result
            
            # Calculate Plazza's selling price with 10% margin (before GST)
            plazza_margin = Decimal('0.10')  # 10% margin
            desired_selling_price_excl_gst = purchase_rate / (1 - plazza_margin)
            
            # Add GST to get final price including GST
            plazza_selling_price_incl_gst = desired_selling_price_excl_gst * (1 + gst_rate)
            
            # Cap at MRP if needed
            plazza_selling_price_incl_gst = min(plazza_selling_price_incl_gst, mrp)
            
            # Calculate customer discount from MRP
            if mrp > 0:
                effective_customer_discount = ((mrp - plazza_selling_price_incl_gst) / mrp) * 100
                
                # Cap discount at 65% maximum
                effective_customer_discount = min(effective_customer_discount, Decimal('65'))
            else:
                effective_customer_discount = Decimal('0')
            
            # Round and store results
            result['plazza_selling_price_incl_gst'] = plazza_selling_price_incl_gst.quantize(
                Decimal('0.01'), rounding=ROUND_HALF_UP)
            
            result['effective_customer_discount'] = effective_customer_discount.quantize(
                Decimal('0.01'), rounding=ROUND_HALF_UP)
            
        except Exception as e:
            self.logger.warning(f"Error calculating price fields: {str(e)}")
        
        return result
    
    def standardize(self, data):
        """
        Standardize the distributor data.
        
        Args:
            data: DataFrame with raw distributor data
            
        Returns:
            Tuple of (standardized_data, error_data)
        """
        # Lowercase column names and strip whitespace
        data.columns = [col.lower().strip() for col in data.columns]
        
        # Apply column mapping
        renamed_columns = {}
        for source, target in self.column_mapping.items():
            if source in data.columns:
                renamed_columns[source] = target
        
        df = data.rename(columns=renamed_columns)
        
        # Create new columns for additional standardized fields
        df['normalized_name'] = None
        df['plazza_selling_price_incl_gst'] = None
        df['effective_customer_discount'] = None
        
        # Process each row
        valid_rows = []
        error_rows = []
        
        for index, row in df.iterrows():
            # Create a new standardized row
            standardized_row = {}
            errors = []
            
            # Process each field
            for field in renamed_columns.values():
                if field in row:
                    value = row[field]
                    
                    # Apply field-specific standardization
                    if field == 'product_name':
                        if pd.isna(value) or value == "":
                            errors.append("Missing product name")
                        else:
                            # Check for placeholder/invalid names
                            product_name = str(value).lower()
                            if len(product_name) <= 2:
                                errors.append("Product name too short")
                            elif any(re.match(pattern, product_name) for pattern in self.placeholder_names):
                                errors.append("Placeholder product name")
                            else:
                                standardized_row[field] = self.normalize_product_name(value)
                                # Also add normalized name for matching
                                standardized_row['normalized_name'] = self.normalize_product_name(value).lower()
                    
                    elif field == 'mrp':
                        # Validate and convert MRP
                        decimal_value = self.safe_decimal(value)
                        if decimal_value is None or decimal_value <= 0:
                            errors.append("Invalid MRP")
                        elif decimal_value > 100000:
                            errors.append("Suspiciously high MRP value")
                        else:
                            standardized_row[field] = decimal_value
                    
                    elif field == 'purchase_rate':
                        # Validate and convert purchase rate
                        decimal_value = self.safe_decimal(value)
                        standardized_row[field] = decimal_value if decimal_value is not None else None
                    
                    elif field == 'gst_percentage':
                        # Validate and convert GST percentage
                        decimal_value = self.safe_decimal(value)
                        if decimal_value is None or decimal_value <= 0:
                            errors.append("Missing or zero GST percentage")
                        elif decimal_value < 0 or decimal_value > 28:
                            errors.append("Invalid GST rate")
                        else:
                            standardized_row[field] = decimal_value
                    
                    elif field == 'package':
                        # Validate and standardize packaging
                        if pd.isna(value) or value == "":
                            errors.append("Missing packaging detail")
                        else:
                            standardized_row[field] = self.standardize_packaging(value)
                    
                    elif field == 'hsn_code':
                        # Validate HSN code
                        valid, error = self.validate_hsn_code(value)
                        if not valid:
                            errors.append(error)
                        standardized_row[field] = str(value).strip() if not pd.isna(value) else None
                    
                    else:
                        # Standard field with no special validation
                        standardized_row[field] = value
            
            # Calculate price fields
            if 'mrp' in standardized_row and 'purchase_rate' in standardized_row and 'gst_percentage' in standardized_row:
                price_fields = self.calculate_price_fields(standardized_row)
                standardized_row.update(price_fields)
            
            # Add original row index for reference
            standardized_row['original_row_index'] = index
            
            # Check if all required fields are present and valid
            missing_required = [field for field in self.required_fields if field not in standardized_row]
            if missing_required:
                errors.append(f"Missing required fields: {', '.join(missing_required)}")
            
            # Add to appropriate list based on validation
            if errors:
                error_row = row.to_dict()
                error_row['errors'] = '; '.join(errors)
                error_rows.append(error_row)
            else:
                valid_rows.append(standardized_row)
        
        # Convert to DataFrames
        valid_df = pd.DataFrame(valid_rows)
        error_df = pd.DataFrame(error_rows)
        
        # Log results
        self.logger.info(f"Standardized {len(valid_df)} records successfully")
        self.logger.info(f"Found {len(error_df)} records with errors")
        
        return valid_df, error_df
```

## Product Name Normalization

### Comprehensive Pharmaceutical Name Normalization
```python
import re

def normalize_product_name(name):
    """
    Normalize pharmaceutical product names for better matching.
    
    This function:
    1. Converts to lowercase
    2. Removes common prefixes (tab, cap, inj, etc.)
    3. Removes dosage units (mg, ml, g, etc.)
    4. Removes packaging information (10x10, 10's, etc.)
    5. Removes parenthetical content
    6. Standardizes spacing and special characters
    
    Args:
        name: The product name to normalize
        
    Returns:
        Normalized product name
    """
    if not name or pd.isna(name):
        return ""
    
    # Convert to lowercase and trim whitespace
    name = str(name).lower().strip()
    
    # Remove special characters except spaces, hyphens, and plus signs
    # (keeping some characters that might be meaningful for drug names)
    name = re.sub(r'[^\w\s\-\+\/]', ' ', name)
    
    # Remove common prefixes if they're at the beginning
    prefixes = ['tab', 'tabs', 'tablet', 'cap', 'caps', 'capsule', 
                'inj', 'injection', 'syp', 'syrup', 'oint', 'gel', 
                'cream', 'lotion', 'drops', 'susp', 'suspension']
    
    pattern = r'^(' + '|'.join(prefixes) + r')\s+'
    name = re.sub(pattern, '', name)
    
    # Remove dosage units with numbers (10mg, 100ml, etc.)
    name = re.sub(r'\b\d+\s*(?:mg|ml|g|mcg|iu|units|gm|milligram|microgram)\b', '', name)
    
    # Remove packaging information (10x10, 10's, etc.)
    name = re.sub(r'\b\d+\s*x\s*\d+\b', '', name)  # 10x10
    name = re.sub(r'\b\d+\s*\'?s\b', '', name)     # 10's
    
    # Remove parenthetical content
    name = re.sub(r'\([^)]*\)', '', name)
    
    # Remove hyphenated dosage info (drug-10mg -> drug)
    name = re.sub(r'-\d+\s*(?:mg|ml|g|mcg|iu)', '', name)
    
    # Remove extra whitespace
    name = re.sub(r'\s+', ' ', name).strip()
    
    return name
```

### Pharmaceutical Product Name Tokenization
```python
def tokenize_product_name(name):
    """
    Tokenize a product name into meaningful components for better matching.
    
    Args:
        name: The product name to tokenize
        
    Returns:
        Dictionary of tokens with categories
    """
    normalized = normalize_product_name(name)
    
    result = {
        'original': name,
        'normalized': normalized,
        'tokens': normalized.split(),
        'brand': None,
        'ingredient': None,
        'dosage': None,
        'form': None
    }
    
    # Extract brand name (usually first word or two)
    if result['tokens']:
        result['brand'] = result['tokens'][0]
        
    # Extract dosage information from original name
    dosage_pattern = r'(\d+(?:\.\d+)?)\s*(?:mg|ml|g|mcg|iu)'
    dosage_match = re.search(dosage_pattern, name.lower())
    if dosage_match:
        result['dosage'] = dosage_match.group(0)
    
    # Extract form information
    forms = ['tablet', 'capsule', 'syrup', 'injection', 'cream', 
             'ointment', 'gel', 'drops', 'suspension', 'powder']
    
    for form in forms:
        if form in name.lower():
            result['form'] = form
            break
    
    return result
```

## Product Matching Techniques

### Multi-Algorithm Fuzzy Matching
```python
from rapidfuzz import fuzz, process
import numpy as np

def calculate_multi_algorithm_similarity(source_name, target_name):
    """
    Calculate similarity between two product names using multiple algorithms.
    
    This function combines several fuzzy matching algorithms to get a more
    robust similarity score.
    
    Args:
        source_name: The source product name
        target_name: The target product name to compare against
        
    Returns:
        Composite similarity score (0-1)
    """
    # Normalize both names first
    source_norm = normalize_product_name(source_name)
    target_norm = normalize_product_name(target_name)
    
    # Skip if either is empty after normalization
    if not source_norm or not target_norm:
        return 0.0
    
    # Calculate various fuzzy match scores
    ratio_score = fuzz.ratio(source_norm, target_norm) / 100
    partial_ratio_score = fuzz.partial_ratio(source_norm, target_norm) / 100
    token_sort_score = fuzz.token_sort_ratio(source_norm, target_norm) / 100
    token_set_score = fuzz.token_set_ratio(source_norm, target_norm) / 100
    
    # Add substring bonus if one string contains the other
    substring_bonus = 0.0
    if source_norm in target_norm or target_norm in source_norm:
        substring_bonus = 0.2
    
    # Calculate weighted composite score
    # Higher weight on token_set_ratio as it handles word order and extra words well
    composite_score = (
        ratio_score * 0.1 +
        partial_ratio_score * 0.3 +
        token_sort_score * 0.2 +
        token_set_score * 0.4 +
        substring_bonus
    )
    
    return min(composite_score, 1.0)  # Cap at 1.0
```

### Product Matching with Multiple Factors
```python
def match_product_comprehensive(distributor_product, catalogue_products, 
                            mrp_tolerance=0.1, min_score=0.6):
    """
    Match a distributor product with catalogue products using multiple factors.
    
    This function considers:
    1. Product name similarity (with multiple algorithms)
    2. MRP/price proximity
    3. Manufacturer similarity
    4. Package/form similarity
    
    Args:
        distributor_product: Dictionary with distributor product details
        catalogue_products: List of catalogue product dictionaries
        mrp_tolerance: MRP percentage tolerance for matching (0.1 = 10%)
        min_score: Minimum composite score to consider a match
        
    Returns:
        List of matched products with scores, sorted by score descending
    """
    # Extract distributor product attributes
    dist_name = distributor_product.get('product_name', '')
    dist_normalized = normalize_product_name(dist_name)
    dist_mrp = float(distributor_product.get('mrp', 0))
    dist_manufacturer = distributor_product.get('manufacturer', '').lower()
    dist_package = distributor_product.get('package', '').lower()
    
    # Early exit if critical fields are missing
    if not dist_normalized or dist_mrp <= 0:
        return []
    
    matches = []
    
    for cat_product in catalogue_products:
        # Extract catalogue product attributes
        cat_name = cat_product.get('name', '')
        cat_normalized = normalize_product_name(cat_name)
        cat_mrp = float(cat_product.get('mrp', 0))
        cat_manufacturer = cat_product.get('manufacturers', '').lower()
        cat_package = cat_product.get('package_detail', '').lower()
        
        # Skip if critical fields are missing
        if not cat_normalized or cat_mrp <= 0:
            continue
        
        # Calculate name similarity (50% weight)
        name_similarity = calculate_multi_algorithm_similarity(
            dist_normalized, cat_normalized)
        
        # Calculate MRP similarity (20% weight)
        # Skip price check if either price is zero
        if dist_mrp > 0 and cat_mrp > 0:
            price_diff = abs(dist_mrp - cat_mrp)
            relative_diff = price_diff / max(dist_mrp, cat_mrp)
            mrp_similarity = max(0, 1 - min(relative_diff / mrp_tolerance, 1))
        else:
            mrp_similarity = 0.5  # Neutral score if we can't compare
        
        # Calculate manufacturer similarity (15% weight)
        if dist_manufacturer and cat_manufacturer:
            manufacturer_similarity = fuzz.token_set_ratio(
                dist_manufacturer, cat_manufacturer) / 100
        else:
            manufacturer_similarity = 0.5  # Neutral score
        
        # Calculate package similarity (15% weight) 
        if dist_package and cat_package:
            package_similarity = fuzz.token_set_ratio(
                dist_package, cat_package) / 100
        else:
            package_similarity = 0.5  # Neutral score
        
        # Calculate composite score with weights
        composite_score = (
            name_similarity * 0.5 +
            mrp_similarity * 0.2 +
            manufacturer_similarity * 0.15 +
            package_similarity * 0.15
        )
        
        # Add to matches if above threshold
        if composite_score >= min_score:
            matches.append({
                'catalogue_id': cat_product.get('product_id', ''),
                'catalogue_name': cat_name,
                'score': composite_score,
                'name_score': name_similarity,
                'mrp_score': mrp_similarity,
                'manufacturer_score': manufacturer_similarity,
                'package_score': package_similarity,
                'dist_mrp': dist_mrp,
                'cat_mrp': cat_mrp
            })
    
    # Sort by score descending
    matches.sort(key=lambda x: x['score'], reverse=True)
    
    return matches
```

### Confidence-Based Matching Classification
```python
def classify_match_confidence(matches):
    """
    Classify matches into confidence levels (high, medium, low).
    
    Args:
        matches: List of match results from match_product_comprehensive
        
    Returns:
        Dictionary with matches grouped by confidence level
    """
    high_confidence = []
    medium_confidence = []
    low_confidence = []
    
    for match in matches:
        score = match['score']
        name_score = match['name_score']
        mrp_score = match['mrp_score']
        
        # High confidence matches
        if score >= 0.9 or (score >= 0.8 and name_score >= 0.9 and mrp_score >= 0.9):
            high_confidence.append(match)
        
        # Medium confidence matches
        elif score >= 0.7 or (score >= 0.6 and name_score >= 0.7 and mrp_score >= 0.8):
            medium_confidence.append(match)
        
        # Low confidence matches
        else:
            low_confidence.append(match)
    
    return {
        'high_confidence': high_confidence,
        'medium_confidence': medium_confidence,
        'low_confidence': low_confidence,
        'best_match': matches[0] if matches else None
    }
```

## Database Access for Product Matching

### CockroachDB Product Query
```python
import psycopg2
import psycopg2.extras
import os

def get_product_data_for_matching(conn, limit=None, offset=0):
    """
    Get product data from CockroachDB for matching purposes.
    
    Args:
        conn: Database connection
        limit: Optional limit on number of records
        offset: Offset for pagination
        
    Returns:
        List of product dictionaries
    """
    query = """
        SELECT 
            product_id,
            product_name,
            manufacturers,
            mrp,
            selling_price,
            packaging_detail,
            prescription_required
        FROM 
            all_products
        ORDER BY 
            product_id
    """
    
    # Add limit and offset if provided
    if limit is not None:
        query += f" LIMIT {limit} OFFSET {offset}"
    
    with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cursor:
        cursor.execute(query)
        products = cursor.fetchall()
    
    return products
```

### Batch Processing for Matching
```python
def batch_match_distributor_to_catalogue(distributor_products, catalogue_products, 
                                         batch_size=1000, min_score=0.6):
    """
    Match distributor products to catalogue in batches.
    
    Args:
        distributor_products: List of distributor products
        catalogue_products: List of catalogue products
        batch_size: Size of batches to process
        min_score: Minimum score for a match
        
    Returns:
        List of match results
    """
    total = len(distributor_products)
    matches = []
    
    print(f"Matching {total} distributor products in batches of {batch_size}")
    
    for i in range(0, total, batch_size):
        batch = distributor_products[i:i+batch_size]
        print(f"Processing batch {i//batch_size + 1}/{(total-1)//batch_size + 1} "
              f"({i}-{min(i+batch_size-1, total-1)})")
        
        batch_matches = []
        for idx, product in enumerate(batch):
            if idx % 100 == 0:
                print(f"  Processing product {i+idx}/{total}")
            
            # Match this product
            product_matches = match_product_comprehensive(
                product, catalogue_products, min_score=min_score)
            
            # If we found matches, add them with the distributor product info
            if product_matches:
                batch_matches.append({
                    'distributor_id': product.get('id', ''),
                    'distributor_item_code': product.get('item_code', ''),
                    'distributor_name': product.get('product_name', ''),
                    'matches': product_matches
                })
        
        matches.extend(batch_matches)
        print(f"  Found {len(batch_matches)} matches in this batch")
    
    print(f"Completed matching with {len(matches)} total matches")
    return matches
```

## Data Validation and Cleaning

### Item Code Standardization and Validation
```python
def standardize_item_codes(products, distributor_field='distributor'):
    """
    Standardize item codes to ensure uniqueness across distributors.
    
    Args:
        products: DataFrame with product data
        distributor_field: Name of distributor field
        
    Returns:
        DataFrame with standardized item codes
    """
    # Create a copy to avoid modifying the original
    df = products.copy()
    
    # Add column for original item codes
    df['original_item_code'] = df['item_code']
    
    # Create standardized item codes with distributor prefix
    for idx, row in df.iterrows():
        distributor = str(row[distributor_field]).lower().strip()
        item_code = str(row['item_code']).strip()
        
        # Skip if either is missing
        if not distributor or not item_code:
            continue
        
        # Remove spaces and special characters from distributor name
        distributor_prefix = re.sub(r'[^a-z0-9]', '', distributor)
        
        # Create prefixed item code if not already prefixed
        if not item_code.lower().startswith(distributor_prefix):
            df.at[idx, 'item_code'] = f"{distributor_prefix}_{item_code}"
    
    return df
```

### Duplicate Detection and Resolution
```python
def identify_duplicates(df, fields):
    """
    Identify duplicate records based on specified fields.
    
    Args:
        df: DataFrame with product data
        fields: List of fields to check for duplicates
        
    Returns:
        DataFrame with duplicate records
    """
    # Find duplicates
    dupes = df.duplicated(subset=fields, keep=False)
    
    # Return duplicate records
    return df[dupes].sort_values(by=fields)

def resolve_item_code_duplicates(df):
    """
    Resolve duplicate item codes by adding sequential suffixes.
    
    Args:
        df: DataFrame with product data
        
    Returns:
        DataFrame with resolved item codes
    """
    # Create a copy
    resolved_df = df.copy()
    
    # Find duplicates
    item_code_counts = df['item_code'].value_counts()
    duplicate_codes = item_code_counts[item_code_counts > 1].index.tolist()
    
    # Process each duplicate item code
    for code in duplicate_codes:
        # Get records with this code
        dupes_idx = df[df['item_code'] == code].index
        
        # Add suffix to each duplicate
        for i, idx in enumerate(dupes_idx):
            if i > 0:  # Keep first occurrence unchanged
                resolved_df.at[idx, 'item_code'] = f"{code}_{i}"
    
    return resolved_df
```

### Data Quality Assessment
```python
def assess_data_quality(df):
    """
    Assess the quality of a DataFrame.
    
    Args:
        df: DataFrame to assess
        
    Returns:
        Dictionary with quality metrics
    """
    metrics = {
        'total_records': len(df),
        'null_counts': {},
        'value_counts': {},
        'stats': {},
        'outliers': {},
        'duplicates': {}
    }
    
    # Check for null values
    for col in df.columns:
        null_count = df[col].isna().sum()
        if null_count > 0:
            metrics['null_counts'][col] = null_count
    
    # Get value counts for categorical columns
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns
    for col in categorical_cols:
        if df[col].nunique() < 100:  # Only for columns with reasonable number of values
            metrics['value_counts'][col] = df[col].value_counts().to_dict()
    
    # Get statistics for numeric columns
    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
    for col in numeric_cols:
        metrics['stats'][col] = {
            'min': df[col].min(),
            'max': df[col].max(),
            'mean': df[col].mean(),
            'median': df[col].median(),
            'std': df[col].std()
        }
        
        # Check for outliers (values more than 3 standard deviations from mean)
        mean = df[col].mean()
        std = df[col].std()
        if std > 0:
            outliers = df[(df[col] > mean + 3*std) | (df[col] < mean - 3*std)]
            if len(outliers) > 0:
                metrics['outliers'][col] = len(outliers)
    
    # Check for duplicate rows
    full_duplicates = df.duplicated().sum()
    if full_duplicates > 0:
        metrics['duplicates']['full_row'] = full_duplicates
    
    # Check for duplicates in key fields
    key_fields = ['product_name', 'item_code', 'manufacturer']
    for field in key_fields:
        if field in df.columns:
            field_duplicates = df.duplicated(subset=[field]).sum()
            if field_duplicates > 0:
                metrics['duplicates'][field] = field_duplicates
    
    return metrics
```

## CSV Processing and Transformation

### CSV Reading with Error Handling
```python
def read_csv_safely(file_path, **kwargs):
    """
    Read a CSV file with comprehensive error handling.
    
    Args:
        file_path: Path to CSV file
        **kwargs: Additional arguments for pd.read_csv
        
    Returns:
        DataFrame or None if error
    """
    try:
        # Set default parameters for robust CSV reading
        params = {
            'low_memory': False,
            'encoding': 'utf-8',
            'engine': 'python',
            'error_bad_lines': False,
            'warn_bad_lines': True
        }
        
        # Update with user parameters
        params.update(kwargs)
        
        # Try reading with specified parameters
        df = pd.read_csv(file_path, **params)
        
        print(f"Successfully read {len(df)} rows from {file_path}")
        return df
    
    except Exception as e:
        print(f"Error reading {file_path}: {str(e)}")
        
        # If UTF-8 encoding fails, try with different encodings
        if 'encoding' in str(e):
            for encoding in ['latin1', 'iso-8859-1', 'cp1252']:
                try:
                    print(f"Trying with {encoding} encoding...")
                    params['encoding'] = encoding
                    df = pd.read_csv(file_path, **params)
                    print(f"Successfully read {len(df)} rows with {encoding} encoding")
                    return df
                except Exception as e2:
                    print(f"Failed with {encoding} encoding: {str(e2)}")
        
        return None
```

### CSV Transformation and Writing
```python
def transform_and_write_csv(input_file, output_file, transformations, **kwargs):
    """
    Read a CSV, apply transformations, and write to a new file.
    
    Args:
        input_file: Path to input CSV
        output_file: Path to output CSV
        transformations: Dict of column_name: transformation_function pairs
        **kwargs: Additional arguments for pd.read_csv
        
    Returns:
        True if successful, False otherwise
    """
    # Read input file
    df = read_csv_safely(input_file, **kwargs)
    if df is None:
        return False
    
    # Apply transformations
    for column, transform_func in transformations.items():
        if column in df.columns:
            df[column] = df[column].apply(transform_func)
    
    # Write output file
    try:
        df.to_csv(output_file, index=False)
        print(f"Successfully wrote {len(df)} rows to {output_file}")
        return True
    except Exception as e:
        print(f"Error writing to {output_file}: {str(e)}")
        return False
```

## Product ID Generation and Management

### UUID Generation with Deterministic Option
```python
import uuid

def generate_uuid(source_id=None, deterministic=False, namespace=None):
    """
    Generate a UUID for product ID.
    
    Args:
        source_id: Optional source ID to use for deterministic UUID
        deterministic: Whether to generate deterministic UUID
        namespace: Namespace to use for deterministic UUID
        
    Returns:
        UUID as string
    """
    if deterministic and source_id:
        # Use UUID5 with namespace for deterministic generation
        if namespace is None:
            namespace = uuid.NAMESPACE_DNS
        
        # Ensure source_id is a string
        source_id_str = str(source_id)
        
        # Generate deterministic UUID based on namespace and source_id
        return str(uuid.uuid5(namespace, source_id_str))
    else:
        # Generate random UUID
        return str(uuid.uuid4())
```

### Item Code to Product ID Mapping
```python
def create_product_id_mapping(conn, source_table, id_field, target_table, target_id_field):
    """
    Create a mapping between item codes and product IDs.
    
    Args:
        conn: Database connection
        source_table: Table containing source IDs
        id_field: Field containing source IDs
        target_table: Table containing target IDs
        target_id_field: Field containing target IDs
        
    Returns:
        Dictionary mapping source IDs to target IDs
    """
    mapping = {}
    
    query = f"""
        SELECT s.{id_field} as source_id, t.{target_id_field} as target_id
        FROM {source_table} s
        JOIN {target_table} t ON s.{id_field} = t.{id_field}
        WHERE s.{id_field} IS NOT NULL AND t.{target_id_field} IS NOT NULL
    """
    
    with conn.cursor(cursor_factory=psycopg2.extras.DictCursor) as cursor:
        cursor.execute(query)
        rows = cursor.fetchall()
        
        for row in rows:
            mapping[row['source_id']] = row['target_id']
    
    return mapping
```

## Matcher Class for Product Matching

```python
class ProductMatcher:
    """
    Class for matching products between distributor data and catalogue.
    """
    
    def __init__(self, db_conn, es_client, mrp_tolerance=0.1, min_score=0.6):
        """
        Initialize the product matcher.
        
        Args:
            db_conn: Database connection
            es_client: Elasticsearch client
            mrp_tolerance: MRP percentage tolerance for matching
            min_score: Minimum score for a match
        """
        self.db_conn = db_conn
        self.es_client = es_client
        self.mrp_tolerance = mrp_tolerance
        self.min_score = min_score
        
        # Load catalogue products for matching
        self.catalogue_products = self._load_catalogue_products()
    
    def _load_catalogue_products(self):
        """Load catalogue products from database"""
        products = get_product_data_for_matching(self.db_conn)
        print(f"Loaded {len(products)} catalogue products for matching")
        return products
    
    def match_distributor_product(self, distributor_product):
        """
        Match a single distributor product to the catalogue.
        
        Args:
            distributor_product: Dictionary with distributor product data
            
        Returns:
            Dictionary with match results
        """
        matches = match_product_comprehensive(
            distributor_product, 
            self.catalogue_products,
            mrp_tolerance=self.mrp_tolerance,
            min_score=self.min_score
        )
        
        # Classify matches by confidence level
        classified = classify_match_confidence(matches)
        
        return {
            'distributor_id': distributor_product.get('id', ''),
            'distributor_item_code': distributor_product.get('item_code', ''),
            'distributor_name': distributor_product.get('product_name', ''),
            'matches': matches,
            'classified': classified,
            'best_match': classified['best_match']
        }
    
    def match_distributor_batch(self, distributor_products, batch_size=1000):
        """
        Match a batch of distributor products to the catalogue.
        
        Args:
            distributor_products: List of distributor product dictionaries
            batch_size: Size of batches to process
            
        Returns:
            List of match results
        """
        return batch_match_distributor_to_catalogue(
            distributor_products,
            self.catalogue_products,
            batch_size=batch_size,
            min_score=self.min_score
        )
    
    def update_elasticsearch_with_matches(self, matches, index_name="distributor-master-list"):
        """
        Update Elasticsearch documents with match information.
        
        Args:
            matches: List of match results
            index_name: Elasticsearch index name
            
        Returns:
            Dictionary with update statistics
        """
        updated = 0
        errors = 0
        
        for match_result in matches:
            # Skip if no best match
            if not match_result.get('best_match'):
                continue
            
            distributor_id = match_result['distributor_id']
            best_match = match_result['best_match']
            catalogue_id = best_match['catalogue_id']
            score = best_match['score']
            
            try:
                # Update document in Elasticsearch
                update_body = {
                    "doc": {
                        "product_id": catalogue_id,
                        "match_score": score,
                        "updated_at": datetime.now().isoformat()
                    }
                }
                
                self.es_client.update(
                    index=index_name,
                    id=distributor_id,
                    body=update_body
                )
                
                updated += 1
            except Exception as e:
                print(f"Error updating document {distributor_id}: {str(e)}")
                errors += 1
        
        return {
            'updated': updated,
            'errors': errors
        }
```

## Data Analysis and Visualization

### Distributor Product Analysis
```python
import matplotlib.pyplot as plt
import seaborn as sns

def analyze_distributor_data(df):
    """
    Analyze distributor product data with visualizations.
    
    Args:
        df: DataFrame with standardized distributor data
        
    Returns:
        Dictionary with analysis results
    """
    results = {
        'total_products': len(df),
        'distributors': df['distributor'].value_counts().to_dict(),
        'price_stats': {},
        'common_manufacturers': df['manufacturer'].value_counts().head(20).to_dict(),
        'hsn_coverage': (df['hsn_code'].notna().sum() / len(df)) * 100
    }
    
    # Price statistics
    for price_field in ['mrp', 'purchase_rate', 'plazza_selling_price_incl_gst']:
        if price_field in df.columns:
            results['price_stats'][price_field] = {
                'min': df[price_field].min(),
                'max': df[price_field].max(),
                'mean': df[price_field].mean(),
                'median': df[price_field].median()
            }
    
    # Distribution of effective discount
    if 'effective_customer_discount' in df.columns:
        results['discount_stats'] = {
            'min': df['effective_customer_discount'].min(),
            'max': df['effective_customer_discount'].max(),
            'mean': df['effective_customer_discount'].mean(),
            'median': df['effective_customer_discount'].median(),
            'distribution': df['effective_customer_discount'].value_counts(bins=10).to_dict()
        }
        
        # Create discount distribution plot
        plt.figure(figsize=(10, 6))
        sns.histplot(df['effective_customer_discount'].dropna(), bins=20, kde=True)
        plt.title('Distribution of Customer Discounts')
        plt.xlabel('Discount Percentage')
        plt.ylabel('Count')
        plt.savefig('discount_distribution.png')
        plt.close()
    
    # Price distribution by distributor
    if 'distributor' in df.columns and 'mrp' in df.columns:
        plt.figure(figsize=(12, 8))
        sns.boxplot(x='distributor', y='mrp', data=df)
        plt.title('MRP Distribution by Distributor')
        plt.xticks(rotation=45)
        plt.savefig('mrp_by_distributor.png')
        plt.close()
    
    return results
```

### Match Results Analysis
```python
def analyze_match_results(matches):
    """
    Analyze product matching results.
    
    Args:
        matches: List of match results from matcher
        
    Returns:
        Dictionary with analysis results
    """
    total_products = len(matches)
    products_with_matches = sum(1 for m in matches if m.get('matches'))
    match_rate = (products_with_matches / total_products) * 100 if total_products > 0 else 0
    
    # Count matches by confidence level
    high_confidence = 0
    medium_confidence = 0
    low_confidence = 0
    
    for match in matches:
        if 'classified' in match and match['classified']:
            if match['classified'].get('high_confidence'):
                high_confidence += 1
            elif match['classified'].get('medium_confidence'):
                medium_confidence += 1
            elif match['classified'].get('low_confidence'):
                low_confidence += 1
    
    # Get score distribution
    scores = []
    for match in matches:
        if match.get('best_match'):
            scores.append(match['best_match']['score'])
    
    # Create score distribution plot
    if scores:
        plt.figure(figsize=(10, 6))
        sns.histplot(scores, bins=20, kde=True)
        plt.title('Distribution of Match Scores')
        plt.xlabel('Score')
        plt.ylabel('Count')
        plt.savefig('match_score_distribution.png')
        plt.close()
    
    return {
        'total_products': total_products,
        'products_with_matches': products_with_matches,
        'match_rate': match_rate,
        'confidence_levels': {
            'high': high_confidence,
            'medium': medium_confidence,
            'low': low_confidence
        },
        'score_stats': {
            'min': min(scores) if scores else None,
            'max': max(scores) if scores else None,
            'mean': sum(scores) / len(scores) if scores else None,
            'count': len(scores)
        }
    }
```

## Reporting and Documentation

### Match Results CSV Generation
```python
def generate_match_report_csv(matches, output_file):
    """
    Generate a CSV report of match results.
    
    Args:
        matches: List of match results
        output_file: Path to output CSV file
        
    Returns:
        True if successful, False otherwise
    """
    # Prepare data for CSV
    rows = []
    
    for match in matches:
        # Skip if no matches
        if not match.get('matches'):
            continue
        
        distributor_id = match.get('distributor_id', '')
        distributor_item_code = match.get('distributor_item_code', '')
        distributor_name = match.get('distributor_name', '')
        
        # Add row for best match if available
        if match.get('best_match'):
            best_match = match['best_match']
            rows.append({
                'distributor_id': distributor_id,
                'item_code': distributor_item_code,
                'distributor_product_name': distributor_name,
                'product_id': best_match.get('catalogue_id', ''),
                'catalogue_product_name': best_match.get('catalogue_name', ''),
                'match_score': best_match.get('score', 0),
                'name_score': best_match.get('name_score', 0),
                'mrp_score': best_match.get('mrp_score', 0),
                'confidence_level': 'High' if match.get('classified', {}).get('high_confidence') else
                                  ('Medium' if match.get('classified', {}).get('medium_confidence') else 'Low')
            })
    
    # Write to CSV
    if rows:
        df = pd.DataFrame(rows)
        df.to_csv(output_file, index=False)
        print(f"Wrote {len(rows)} matches to {output_file}")
        return True
    else:
        print("No matches to write")
        return False
```

### Markdown Documentation Generation
```python
def generate_matching_documentation(analysis_results, output_file):
    """
    Generate markdown documentation of matching results.
    
    Args:
        analysis_results: Results from analyze_match_results
        output_file: Path to output markdown file
        
    Returns:
        True if successful, False otherwise
    """
    try:
        with open(output_file, 'w') as f:
            # Write header
            f.write("# Product Matching Results Summary\n\n")
            f.write(f"**Generated on:** {datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n")
            
            # Write overview
            f.write("## Overview\n\n")
            f.write(f"- **Total products processed:** {analysis_results['total_products']}\n")
            f.write(f"- **Products with matches:** {analysis_results['products_with_matches']} "
                   f"({analysis_results['match_rate']:.2f}%)\n")
            
            # Write confidence levels
            f.write("\n## Match Confidence Levels\n\n")
            f.write(f"- **High confidence matches:** {analysis_results['confidence_levels']['high']}\n")
            f.write(f"- **Medium confidence matches:** {analysis_results['confidence_levels']['medium']}\n")
            f.write(f"- **Low confidence matches:** {analysis_results['confidence_levels']['low']}\n")
            
            # Write score statistics
            f.write("\n## Score Statistics\n\n")
            f.write(f"- **Minimum score:** {analysis_results['score_stats']['min']:.2f}\n")
            f.write(f"- **Maximum score:** {analysis_results['score_stats']['max']:.2f}\n")
            f.write(f"- **Mean score:** {analysis_results['score_stats']['mean']:.2f}\n")
            
            # Write next steps
            f.write("\n## Next Steps\n\n")
            f.write("1. Review the match results CSV file for accuracy\n")
            f.write("2. Manually validate medium and low confidence matches\n")
            f.write("3. Update the Elasticsearch index with validated matches\n")
            f.write("4. Analyze unmatched products to identify patterns\n")
            
            # Write image references if available
            f.write("\n## Visualizations\n\n")
            f.write("### Match Score Distribution\n\n")
            f.write("![Match Score Distribution](match_score_distribution.png)\n")
            
        print(f"Generated documentation at {output_file}")
        return True
    
    except Exception as e:
        print(f"Error generating documentation: {str(e)}")
        return False
```