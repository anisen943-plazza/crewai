# Data Synchronization Patterns Knowledge Base - 2025-03-26

## Metadata
- **Documentation Date**: 2025-03-26
- **Documentation Type**: Technical
- **Data Domains**: Data Synchronization, System Integration, ETL
- **Generated By**: Claude Code

## Cross-System Synchronization Architecture

### System Architecture Overview
```
┌────────────────┐      ┌────────────────┐      ┌────────────────┐
│                │      │                │      │                │
│    Airtable    │─────▶│   CockroachDB  │─────▶│  Elasticsearch │
│                │      │                │      │                │
└────────────────┘      └────────────────┘      └────────────────┘
        │                       ▲                       ▲
        │                       │                       │
        │                       │                       │
        │               ┌───────┴───────┐       ┌───────┴───────┐
        └───────────────▶  Distributor  │───────▶Product Matching│
                        │     CSV       │       │    System     │
                        └───────────────┘       └───────────────┘
```

In this architecture:
1. Airtable serves as the source of truth for product catalog data
2. Distributor data from CSV files is standardized and imported
3. CockroachDB serves as the central data repository
4. Elasticsearch provides search and retrieval capabilities
5. The Product Matching System links distributor products to the main catalog

### Core Principles of Synchronization
1. **Unidirectional Flow**: Data flows in one direction (Airtable → CockroachDB → Elasticsearch)
2. **Incremental Updates**: Only changed records are processed when possible
3. **Consistent IDs**: UUID-based IDs provide consistent reference across systems
4. **Timestamp Tracking**: All operations track last sync time for incremental updates
5. **Error Isolation**: Errors in one record don't prevent processing of others
6. **Validation**: Data is validated before insertion into each system
7. **Transformation**: Data is transformed appropriately for each target system

## Airtable to CockroachDB Synchronization

### Incremental Sync Pattern
```python
import os
import psycopg2
import pyairtable
from datetime import datetime, timedelta
import logging

class AirtableToCockroachSync:
    """Class for synchronizing data from Airtable to CockroachDB."""
    
    def __init__(self, airtable_base_id, airtable_api_key, db_url):
        """Initialize the synchronizer with connection parameters."""
        self.airtable_base_id = airtable_base_id
        self.airtable_api_key = airtable_api_key
        self.db_url = db_url
        
        # Setup logging
        self.logger = logging.getLogger('airtable_sync')
        self.logger.setLevel(logging.INFO)
        
        # Ensure we have a console handler
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
    
    def get_last_sync_time(self, table_name):
        """
        Get the timestamp of the last sync for a given table.
        
        Args:
            table_name: Name of the table to check
            
        Returns:
            Datetime of last sync, or None if no previous sync
        """
        conn = None
        try:
            # Connect to database
            conn = psycopg2.connect(self.db_url)
            cursor = conn.cursor()
            
            # Check if sync_control table exists
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_name = 'sync_control'
                )
            """)
            table_exists = cursor.fetchone()[0]
            
            if not table_exists:
                # Create sync_control table if it doesn't exist
                cursor.execute("""
                    CREATE TABLE sync_control (
                        source_table VARCHAR(255) PRIMARY KEY,
                        last_sync_time TIMESTAMP NOT NULL,
                        records_processed INTEGER NOT NULL DEFAULT 0,
                        created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                        updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                conn.commit()
                return None
            
            # Query last sync time
            cursor.execute("""
                SELECT last_sync_time FROM sync_control
                WHERE source_table = %s
            """, (table_name,))
            
            result = cursor.fetchone()
            return result[0] if result else None
            
        except Exception as e:
            self.logger.error(f"Error getting last sync time: {str(e)}")
            return None
        finally:
            if conn:
                conn.close()
    
    def update_sync_time(self, table_name, records_processed):
        """
        Update the sync_control table with the current time.
        
        Args:
            table_name: Name of the table that was synced
            records_processed: Number of records processed
        """
        conn = None
        try:
            # Connect to database
            conn = psycopg2.connect(self.db_url)
            cursor = conn.cursor()
            
            now = datetime.now()
            
            # Insert or update the sync_control record
            cursor.execute("""
                INSERT INTO sync_control 
                    (source_table, last_sync_time, records_processed, updated_at)
                VALUES 
                    (%s, %s, %s, %s)
                ON CONFLICT (source_table) 
                DO UPDATE SET 
                    last_sync_time = EXCLUDED.last_sync_time,
                    records_processed = sync_control.records_processed + EXCLUDED.records_processed,
                    updated_at = EXCLUDED.updated_at
            """, (table_name, now, records_processed, now))
            
            conn.commit()
            self.logger.info(f"Updated sync time for {table_name} with {records_processed} records")
            
        except Exception as e:
            self.logger.error(f"Error updating sync time: {str(e)}")
        finally:
            if conn:
                conn.close()
    
    def fetch_airtable_records(self, table_name, table_id, last_sync=None):
        """
        Fetch records from Airtable, optionally filtering by last modified time.
        
        Args:
            table_name: Descriptive name of the table
            table_id: Airtable table ID
            last_sync: Optional datetime to filter by last modified time
            
        Returns:
            List of Airtable records
        """
        try:
            # Initialize Airtable client
            table = pyairtable.Table(self.airtable_api_key, self.airtable_base_id, table_id)
            
            # Prepare filter formula if we have a last sync time
            formula = None
            if last_sync:
                # Format time for Airtable filtering
                airtable_time_format = last_sync.strftime('%Y-%m-%dT%H:%M:%S.000Z')
                formula = f"LAST_MODIFIED_TIME() > '{airtable_time_format}'"
                self.logger.info(f"Fetching records modified after {airtable_time_format}")
            
            # Fetch records with or without filter
            if formula:
                records = table.all(formula=formula)
            else:
                records = table.all()
            
            self.logger.info(f"Fetched {len(records)} records from {table_name}")
            return records
            
        except Exception as e:
            self.logger.error(f"Error fetching Airtable records: {str(e)}")
            return []
    
    def prepare_upsert_query(self, table_name, record, primary_key):
        """
        Prepare an upsert query for a given record.
        
        Args:
            table_name: Database table name
            record: Dictionary of field:value pairs to insert
            primary_key: Name of the primary key field
            
        Returns:
            Tuple of (query, parameters)
        """
        # Extract fields and values
        fields = list(record.keys())
        values = list(record.values())
        
        # Create placeholders for values
        placeholders = [f"%s" for _ in fields]
        
        # Prepare column list and placeholder string
        columns = ", ".join([f'"{field}"' for field in fields])
        placeholder_str = ", ".join(placeholders)
        
        # Prepare SET clause for UPDATE
        update_clause = []
        for field in fields:
            # Skip primary key
            if field == primary_key:
                continue
            update_clause.append(f'"{field}" = EXCLUDED."{field}"')
        
        update_sql = ", ".join(update_clause)
        
        # Build complete query
        sql = f"""
            INSERT INTO {table_name} ({columns})
            VALUES ({placeholder_str})
            ON CONFLICT ("{primary_key}") 
            DO UPDATE SET {update_sql}
        """
        
        return sql, values
    
    def transform_airtable_record(self, record, field_mapping):
        """
        Transform an Airtable record to match the database schema.
        
        Args:
            record: Airtable record dictionary
            field_mapping: Dictionary mapping Airtable fields to database fields
            
        Returns:
            Transformed record dictionary
        """
        transformed = {}
        
        # Extract fields dict from Airtable record
        airtable_fields = record.get('fields', {})
        
        # Apply field mapping
        for airtable_field, db_field in field_mapping.items():
            if airtable_field in airtable_fields:
                value = airtable_fields[airtable_field]
                
                # Handle array values (common in Airtable)
                if isinstance(value, list):
                    if value:  # If the list has items
                        value = value[0]  # Take the first item
                    else:
                        value = None  # Empty list becomes None
                
                transformed[db_field] = value
        
        # Add metadata fields
        transformed['airtable_id'] = record.get('id')
        transformed['source_system'] = 'airtable'
        transformed['synced_at'] = datetime.now()
        
        return transformed
    
    def sync_table(self, airtable_table_name, airtable_table_id, db_table_name, 
                  field_mapping, primary_key, force_refresh=False):
        """
        Synchronize a single table from Airtable to CockroachDB.
        
        Args:
            airtable_table_name: Descriptive name of the Airtable table
            airtable_table_id: Airtable table ID
            db_table_name: Database table name
            field_mapping: Dictionary mapping Airtable fields to database fields
            primary_key: Name of the primary key field in the database
            force_refresh: Whether to force a full refresh regardless of last sync time
            
        Returns:
            Number of records processed
        """
        # Get last sync time if not forcing refresh
        last_sync = None if force_refresh else self.get_last_sync_time(db_table_name)
        
        self.logger.info(f"Starting sync of {airtable_table_name} to {db_table_name}")
        if last_sync:
            self.logger.info(f"Last sync time: {last_sync}")
        else:
            self.logger.info("No previous sync found, will sync all records")
        
        # Fetch records from Airtable
        records = self.fetch_airtable_records(airtable_table_name, airtable_table_id, last_sync)
        if not records:
            self.logger.info(f"No records to sync for {airtable_table_name}")
            return 0
        
        # Process records
        conn = None
        processed_count = 0
        
        try:
            # Connect to database
            conn = psycopg2.connect(self.db_url)
            cursor = conn.cursor()
            
            # Process each record
            for record in records:
                try:
                    # Transform the record
                    transformed = self.transform_airtable_record(record, field_mapping)
                    
                    # Prepare and execute upsert query
                    sql, values = self.prepare_upsert_query(db_table_name, transformed, primary_key)
                    cursor.execute(sql, values)
                    
                    processed_count += 1
                    
                    # Log progress periodically
                    if processed_count % 100 == 0:
                        self.logger.info(f"Processed {processed_count}/{len(records)} records")
                    
                except Exception as e:
                    # Log error but continue with next record
                    self.logger.error(f"Error processing record {record.get('id')}: {str(e)}")
            
            # Commit changes
            conn.commit()
            self.logger.info(f"Committed {processed_count} records to {db_table_name}")
            
            # Update sync time
            self.update_sync_time(db_table_name, processed_count)
            
            return processed_count
            
        except Exception as e:
            self.logger.error(f"Error syncing table {airtable_table_name}: {str(e)}")
            if conn:
                conn.rollback()
            return 0
        finally:
            if conn:
                conn.close()
```

### UUID Handling for Foreign Keys
```python
import uuid

def generate_deterministic_uuid(string_id, namespace=uuid.NAMESPACE_DNS):
    """
    Generate a deterministic UUID based on a string identifier.
    
    Args:
        string_id: String to generate UUID from
        namespace: UUID namespace for consistency
        
    Returns:
        UUID as string
    """
    return str(uuid.uuid5(namespace, str(string_id)))

def transform_record_with_uuids(record, airtable_table_name, db_table_name, contact_id_mapping=None):
    """
    Transform Airtable record with proper UUID handling.
    
    Args:
        record: Airtable record
        airtable_table_name: Airtable table name (for field mapping lookup)
        db_table_name: Database table name
        contact_id_mapping: Optional mapping of contact IDs to UUIDs
        
    Returns:
        Transformed record with UUIDs
    """
    transformed = {}
    field_mapping = FIELD_MAPPINGS.get(db_table_name, {})
    
    # Extract fields dict from Airtable record
    airtable_fields = record.get('fields', {})
    airtable_id = record.get('id')
    
    # Apply field mapping
    for airtable_field, db_field in field_mapping.items():
        if airtable_field in airtable_fields:
            value = airtable_fields[airtable_field]
            
            # Handle array values (common in Airtable)
            if isinstance(value, list):
                if value:  # If the list has items
                    value = value[0]  # Take the first item
                else:
                    value = None  # Empty list becomes None
            
            transformed[db_field] = value
    
    # Add default fields with special handling for IDs
    if "_defaults" in field_mapping:
        for default_field, default_value in field_mapping["_defaults"].items():
            # Generate random UUID for primary key
            if default_field == "id" and default_value == "generated_uuid":
                transformed[default_field] = str(uuid.uuid4())
            
            # Generate deterministic UUID from ContactID
            elif default_field == "contact_id" and default_value == "generated_uuid_from_contact_id":
                if "ContactID" in airtable_fields:
                    contact_id_value = airtable_fields["ContactID"]
                    
                    # First try to use the contact_id_mapping if available
                    if contact_id_mapping and (airtable_id in contact_id_mapping or 
                                              str(contact_id_value) in contact_id_mapping):
                        if airtable_id in contact_id_mapping:
                            transformed[default_field] = contact_id_mapping[airtable_id]
                        elif str(contact_id_value) in contact_id_mapping:
                            transformed[default_field] = contact_id_mapping[str(contact_id_value)]
                    else:
                        # Fallback to generating a deterministic UUID
                        transformed[default_field] = generate_deterministic_uuid(str(contact_id_value))
            
            # Handle other default fields
            else:
                transformed[default_field] = default_value
    
    # Add metadata fields
    transformed['airtable_id'] = airtable_id
    transformed['source_system'] = 'airtable'
    transformed['synced_at'] = datetime.now()
    
    return transformed
```

### Field Type Transformation and Validation
```python
def transform_field_value(value, db_type, field_name):
    """
    Transform a value to match the target database type with validation.
    
    Args:
        value: Value to transform
        db_type: Target database type
        field_name: Field name for error reporting
        
    Returns:
        Tuple of (transformed_value, is_valid, error_message)
    """
    if value is None:
        return None, True, None
    
    try:
        # UUID type
        if db_type.startswith('uuid'):
            if isinstance(value, str) and len(value) == 36:
                # Validate UUID format
                try:
                    uuid_obj = uuid.UUID(value)
                    return str(uuid_obj), True, None
                except ValueError:
                    return None, False, f"Invalid UUID format for {field_name}: {value}"
            else:
                return None, False, f"Invalid UUID value for {field_name}: {value}"
        
        # Numeric types
        elif db_type in ('integer', 'smallint', 'bigint'):
            return int(value), True, None
        elif db_type.startswith('numeric') or db_type.startswith('decimal'):
            return float(value), True, None
        elif db_type == 'real' or db_type == 'double precision':
            return float(value), True, None
        
        # Date/time types
        elif db_type.startswith('timestamp') or db_type == 'date':
            if isinstance(value, str):
                # Try multiple date formats
                for fmt in ('%Y-%m-%dT%H:%M:%S.%fZ', '%Y-%m-%dT%H:%M:%SZ', 
                          '%Y-%m-%d %H:%M:%S', '%Y-%m-%d'):
                    try:
                        return datetime.strptime(value, fmt), True, None
                    except ValueError:
                        continue
                return None, False, f"Invalid date format for {field_name}: {value}"
            elif isinstance(value, (datetime, date)):
                return value, True, None
            else:
                return None, False, f"Invalid date value for {field_name}: {value}"
        
        # Boolean type
        elif db_type == 'boolean':
            if isinstance(value, bool):
                return value, True, None
            elif isinstance(value, str):
                if value.lower() in ('true', 't', 'yes', 'y', '1'):
                    return True, True, None
                elif value.lower() in ('false', 'f', 'no', 'n', '0'):
                    return False, True, None
                else:
                    return None, False, f"Invalid boolean value for {field_name}: {value}"
            elif isinstance(value, (int, float)):
                return bool(value), True, None
            else:
                return None, False, f"Invalid boolean value for {field_name}: {value}"
        
        # Text types
        elif db_type.startswith('varchar') or db_type == 'text':
            # If we have a character limit, validate against it
            max_length = None
            if db_type.startswith('varchar'):
                match = re.search(r'varchar\((\d+)\)', db_type)
                if match:
                    max_length = int(match.group(1))
            
            value_str = str(value)
            
            if max_length and len(value_str) > max_length:
                return value_str[:max_length], False, f"Value too long for {field_name}: {len(value_str)} > {max_length}"
            return value_str, True, None
        
        # Array types
        elif db_type.endswith('[]'):
            if isinstance(value, list):
                return value, True, None
            else:
                try:
                    # Try to convert to list if it's JSON or a string representation
                    if isinstance(value, str):
                        if value.startswith('[') and value.endswith(']'):
                            import json
                            return json.loads(value), True, None
                    # If a single value, make it a list
                    return [value], True, None
                except Exception:
                    return None, False, f"Cannot convert to array for {field_name}: {value}"
        
        # JSON types
        elif db_type == 'json' or db_type == 'jsonb':
            if isinstance(value, (dict, list)):
                return value, True, None
            elif isinstance(value, str):
                try:
                    import json
                    return json.loads(value), True, None
                except json.JSONDecodeError:
                    return None, False, f"Invalid JSON for {field_name}: {value}"
            else:
                return None, False, f"Cannot convert to JSON for {field_name}: {value}"
        
        # Default case - return as is
        else:
            return value, True, None
            
    except Exception as e:
        return None, False, f"Error converting {field_name} to {db_type}: {str(e)}"
```

## CockroachDB to Elasticsearch Synchronization

### Full Table Sync Pattern
```python
from elasticsearch import Elasticsearch, helpers
import psycopg2
import psycopg2.extras
import os
from datetime import datetime
import time

def sync_db_to_elasticsearch(index_name, table_name, id_field, batch_size=1000, 
                             transform_func=None):
    """
    Synchronize data from CockroachDB to Elasticsearch.
    
    Args:
        index_name: Elasticsearch index name
        table_name: Database table name
        id_field: Name of the ID field to use as document ID
        batch_size: Number of records to process in each batch
        transform_func: Optional function to transform records before indexing
        
    Returns:
        Dictionary with statistics about the sync process
    """
    start_time = time.time()
    
    # Connect to database
    db_conn = psycopg2.connect(os.getenv('DATABASE_URL'))
    db_conn.autocommit = True
    
    # Connect to Elasticsearch
    es = Elasticsearch(
        os.getenv('ES_HOST'),
        basic_auth=(os.getenv('ES_USER'), os.getenv('ES_PASSWORD')),
        verify_certs=False
    )
    
    total_docs = 0
    processed_docs = 0
    error_docs = 0
    
    try:
        # Get total count for progress tracking
        with db_conn.cursor() as cursor:
            cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
            total_docs = cursor.fetchone()[0]
            print(f"Found {total_docs} records in {table_name} to sync")
        
        # Process in batches
        with db_conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cursor:
            # Fetch records in batches
            cursor.execute(f"SELECT * FROM {table_name}")
            
            batch_num = 0
            while True:
                batch = cursor.fetchmany(batch_size)
                if not batch:
                    break
                
                batch_num += 1
                batch_size = len(batch)
                print(f"Processing batch {batch_num} with {batch_size} records")
                
                # Prepare actions for bulk indexing
                actions = []
                for record in batch:
                    try:
                        # Transform record if a transform function is provided
                        if transform_func:
                            doc = transform_func(record)
                        else:
                            doc = record
                        
                        # Create action for Elasticsearch
                        action = {
                            '_index': index_name,
                            '_id': str(doc[id_field]),
                            '_source': doc
                        }
                        actions.append(action)
                    except Exception as e:
                        print(f"Error processing record {record.get(id_field, 'unknown')}: {str(e)}")
                        error_docs += 1
                
                # Execute bulk indexing
                if actions:
                    success, errors = helpers.bulk(es, actions, stats_only=True)
                    processed_docs += success
                    error_docs += errors
                    
                    # Log progress
                    progress = (processed_docs / total_docs) * 100 if total_docs > 0 else 0
                    elapsed = time.time() - start_time
                    docs_per_second = processed_docs / elapsed if elapsed > 0 else 0
                    
                    print(f"Processed {processed_docs}/{total_docs} records "
                         f"({progress:.1f}%) - {docs_per_second:.1f} docs/sec")
    
    except Exception as e:
        print(f"Error in sync process: {str(e)}")
    finally:
        db_conn.close()
    
    # Return statistics
    total_time = time.time() - start_time
    return {
        'total_docs': total_docs,
        'processed_docs': processed_docs,
        'error_docs': error_docs,
        'elapsed_seconds': total_time,
        'docs_per_second': processed_docs / total_time if total_time > 0 else 0
    }
```

### Incremental Elasticsearch Sync Pattern
```python
def incremental_sync_to_elasticsearch(index_name, table_name, id_field, timestamp_field,
                                     batch_size=1000, transform_func=None):
    """
    Perform an incremental sync from database to Elasticsearch using timestamp.
    
    Args:
        index_name: Elasticsearch index name
        table_name: Database table name
        id_field: Name of the ID field to use as document ID
        timestamp_field: Name of the timestamp field to use for incremental sync
        batch_size: Number of records to process in each batch
        transform_func: Optional function to transform records before indexing
        
    Returns:
        Dictionary with statistics about the sync process
    """
    start_time = time.time()
    
    # Connect to database
    db_conn = psycopg2.connect(os.getenv('DATABASE_URL'))
    db_conn.autocommit = True
    
    # Connect to Elasticsearch
    es = Elasticsearch(
        os.getenv('ES_HOST'),
        basic_auth=(os.getenv('ES_USER'), os.getenv('ES_PASSWORD')),
        verify_certs=False
    )
    
    # Get last sync time for this table
    last_sync = get_last_sync_time(db_conn, table_name)
    
    print(f"Starting incremental sync for {table_name} to {index_name}")
    if last_sync:
        print(f"Last sync time: {last_sync}")
    else:
        print("No previous sync found, will sync all records")
    
    processed_docs = 0
    error_docs = 0
    
    try:
        # Build query with timestamp filter if we have a last sync time
        query = f"SELECT * FROM {table_name}"
        params = []
        
        if last_sync:
            query += f" WHERE {timestamp_field} > %s"
            params = [last_sync]
        
        query += f" ORDER BY {timestamp_field}"
        
        # Get total count for progress tracking
        count_query = f"SELECT COUNT(*) FROM ({query}) AS subquery"
        with db_conn.cursor() as cursor:
            cursor.execute(count_query, params)
            total_docs = cursor.fetchone()[0]
            print(f"Found {total_docs} records to sync")
        
        # Process in batches
        with db_conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cursor:
            cursor.execute(query, params)
            
            batch_num = 0
            latest_timestamp = last_sync
            
            while True:
                batch = cursor.fetchmany(batch_size)
                if not batch:
                    break
                
                batch_num += 1
                print(f"Processing batch {batch_num} with {len(batch)} records")
                
                # Prepare actions for bulk indexing
                actions = []
                for record in batch:
                    try:
                        # Keep track of latest timestamp
                        record_timestamp = record[timestamp_field]
                        if not latest_timestamp or record_timestamp > latest_timestamp:
                            latest_timestamp = record_timestamp
                        
                        # Transform record if needed
                        if transform_func:
                            doc = transform_func(record)
                        else:
                            doc = record
                        
                        # Create action for Elasticsearch
                        action = {
                            '_index': index_name,
                            '_id': str(doc[id_field]),
                            '_source': doc
                        }
                        actions.append(action)
                    except Exception as e:
                        print(f"Error processing record {record.get(id_field, 'unknown')}: {str(e)}")
                        error_docs += 1
                
                # Execute bulk indexing
                if actions:
                    success, errors = helpers.bulk(es, actions, stats_only=True)
                    processed_docs += success
                    error_docs += errors
                    
                    # Log progress
                    progress = (processed_docs / total_docs) * 100 if total_docs > 0 else 0
                    elapsed = time.time() - start_time
                    docs_per_second = processed_docs / elapsed if elapsed > 0 else 0
                    
                    print(f"Processed {processed_docs}/{total_docs} records "
                         f"({progress:.1f}%) - {docs_per_second:.1f} docs/sec")
            
            # Update sync control with latest timestamp
            if latest_timestamp and latest_timestamp != last_sync:
                update_sync_time(db_conn, table_name, latest_timestamp, processed_docs)
    
    except Exception as e:
        print(f"Error in incremental sync process: {str(e)}")
    finally:
        db_conn.close()
    
    # Return statistics
    total_time = time.time() - start_time
    return {
        'processed_docs': processed_docs,
        'error_docs': error_docs,
        'elapsed_seconds': total_time,
        'docs_per_second': processed_docs / total_time if total_time > 0 else 0,
        'latest_timestamp': latest_timestamp
    }
```

### Field-Specific Synchronization
```python
def sync_specific_fields_to_elasticsearch(index_name, table_name, id_field, field_list,
                                         batch_size=1000):
    """
    Synchronize specific fields from database to Elasticsearch.
    
    Args:
        index_name: Elasticsearch index name
        table_name: Database table name
        id_field: Name of the ID field to use as document ID
        field_list: List of fields to synchronize
        batch_size: Number of records to process in each batch
        
    Returns:
        Dictionary with statistics about the sync process
    """
    start_time = time.time()
    
    # Connect to database
    db_conn = psycopg2.connect(os.getenv('DATABASE_URL'))
    db_conn.autocommit = True
    
    # Connect to Elasticsearch
    es = Elasticsearch(
        os.getenv('ES_HOST'),
        basic_auth=(os.getenv('ES_USER'), os.getenv('ES_PASSWORD')),
        verify_certs=False
    )
    
    fields_str = f"{id_field}, " + ", ".join(field_list)
    query = f"SELECT {fields_str} FROM {table_name}"
    
    total_docs = 0
    processed_docs = 0
    error_docs = 0
    
    try:
        # Get total count for progress tracking
        with db_conn.cursor() as cursor:
            cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
            total_docs = cursor.fetchone()[0]
            print(f"Found {total_docs} records in {table_name} to update fields: {field_list}")
        
        # Process in batches
        with db_conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cursor:
            cursor.execute(query)
            
            batch_num = 0
            
            while True:
                batch = cursor.fetchmany(batch_size)
                if not batch:
                    break
                
                batch_num += 1
                print(f"Processing batch {batch_num} with {len(batch)} records")
                
                # Process each record
                for record in batch:
                    try:
                        # Extract document ID
                        doc_id = str(record[id_field])
                        
                        # Prepare update document (only specified fields)
                        doc = {}
                        for field in field_list:
                            if field in record and record[field] is not None:
                                doc[field] = record[field]
                        
                        # Add update timestamp
                        doc['updated_at'] = datetime.now().isoformat()
                        
                        # Update document
                        es.update(
                            index=index_name,
                            id=doc_id,
                            body={"doc": doc, "doc_as_upsert": False}
                        )
                        
                        processed_docs += 1
                        
                        # Log progress periodically
                        if processed_docs % 100 == 0:
                            progress = (processed_docs / total_docs) * 100
                            elapsed = time.time() - start_time
                            docs_per_second = processed_docs / elapsed if elapsed > 0 else 0
                            
                            print(f"Processed {processed_docs}/{total_docs} records "
                                 f"({progress:.1f}%) - {docs_per_second:.1f} docs/sec")
                    
                    except Exception as e:
                        print(f"Error updating document {record.get(id_field, 'unknown')}: {str(e)}")
                        error_docs += 1
    
    except Exception as e:
        print(f"Error in field sync process: {str(e)}")
    finally:
        db_conn.close()
    
    # Return statistics
    total_time = time.time() - start_time
    return {
        'total_docs': total_docs,
        'processed_docs': processed_docs,
        'error_docs': error_docs,
        'elapsed_seconds': total_time,
        'docs_per_second': processed_docs / total_time if total_time > 0 else 0,
        'fields_updated': field_list
    }
```

## CSV to Database Import Patterns

### CSV Import with Validation and Transformation
```python
import pandas as pd
import psycopg2
from psycopg2.extras import execute_batch
import os
from datetime import datetime
import logging

def import_csv_to_database(csv_file, table_name, column_mapping=None, 
                          transform_func=None, batch_size=1000):
    """
    Import data from a CSV file to a database table.
    
    Args:
        csv_file: Path to the CSV file
        table_name: Name of the target database table
        column_mapping: Optional dictionary mapping CSV columns to database columns
        transform_func: Optional function to transform rows before import
        batch_size: Number of records to insert in each batch
        
    Returns:
        Dictionary with import statistics
    """
    start_time = datetime.now()
    
    # Setup logging
    logger = logging.getLogger('csv_import')
    logger.setLevel(logging.INFO)
    
    if not logger.handlers:
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    
    # Load CSV data
    logger.info(f"Loading data from {csv_file}")
    try:
        df = pd.read_csv(csv_file)
        total_rows = len(df)
        logger.info(f"Loaded {total_rows} rows from CSV")
    except Exception as e:
        logger.error(f"Error loading CSV: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'rows_processed': 0,
            'rows_imported': 0,
            'rows_error': 0
        }
    
    # Apply column mapping if provided
    if column_mapping:
        df = df.rename(columns=column_mapping)
    
    # Connect to the database
    logger.info(f"Connecting to database")
    conn = None
    
    try:
        conn = psycopg2.connect(os.getenv('DATABASE_URL'))
        conn.autocommit = False  # Use explicit transactions
        
        # Get columns from the dataframe that match database table
        with conn.cursor() as cursor:
            # Get table columns
            cursor.execute(f"""
                SELECT column_name
                FROM information_schema.columns
                WHERE table_name = '{table_name}'
                AND table_schema = 'public'
                AND column_name != 'id'  # Skip auto-generated ID column
            """)
            db_columns = [row[0] for row in cursor.fetchall()]
            
            # Filter dataframe columns to match database
            df_columns = [col for col in df.columns if col.lower() in [c.lower() for c in db_columns]]
            
            logger.info(f"Using columns: {df_columns}")
            
            # Prepare placeholders for SQL
            placeholders = ', '.join(['%s'] * len(df_columns))
            columns = ', '.join([f'"{col}"' for col in df_columns])
            
            # Prepare insert query
            insert_query = f"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})"
            
            # Process in batches
            rows_imported = 0
            rows_error = 0
            
            for i in range(0, total_rows, batch_size):
                # Get batch of rows
                batch_df = df.iloc[i:i+batch_size]
                
                # Start transaction
                cursor.execute("BEGIN")
                
                try:
                    # Prepare batch data
                    batch_data = []
                    
                    for _, row in batch_df.iterrows():
                        try:
                            # Apply transformation if provided
                            if transform_func:
                                row = transform_func(row)
                            
                            # Extract values for database columns
                            values = [row[col] if col in row and pd.notna(row[col]) else None 
                                     for col in df_columns]
                            
                            batch_data.append(values)
                        except Exception as e:
                            logger.error(f"Error transforming row: {str(e)}")
                            rows_error += 1
                    
                    # Execute batch insert
                    execute_batch(cursor, insert_query, batch_data)
                    
                    # Commit transaction
                    conn.commit()
                    
                    rows_imported += len(batch_data)
                    logger.info(f"Imported batch {i//batch_size + 1}: {len(batch_data)} rows "
                               f"({rows_imported}/{total_rows}, {rows_error} errors)")
                    
                except Exception as e:
                    # Rollback on error
                    conn.rollback()
                    logger.error(f"Error importing batch {i//batch_size + 1}: {str(e)}")
                    rows_error += len(batch_df)
        
        # Calculate elapsed time
        elapsed = (datetime.now() - start_time).total_seconds()
        
        # Log results
        logger.info(f"Import completed in {elapsed:.2f} seconds")
        logger.info(f"Rows processed: {total_rows}")
        logger.info(f"Rows imported: {rows_imported}")
        logger.info(f"Rows with errors: {rows_error}")
        
        return {
            'success': True,
            'rows_processed': total_rows,
            'rows_imported': rows_imported,
            'rows_error': rows_error,
            'elapsed_seconds': elapsed
        }
    
    except Exception as e:
        logger.error(f"Database error: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'rows_processed': total_rows,
            'rows_imported': 0,
            'rows_error': total_rows
        }
    finally:
        if conn:
            conn.close()
```

### Batch Processing for Large Files
```python
def process_large_csv_in_chunks(csv_file, chunk_size=10000, process_func=None):
    """
    Process a large CSV file in chunks to avoid memory issues.
    
    Args:
        csv_file: Path to the CSV file
        chunk_size: Number of rows to process in each chunk
        process_func: Function to process each chunk
        
    Returns:
        Dictionary with processing statistics
    """
    start_time = datetime.now()
    
    # Setup logging
    logger = logging.getLogger('csv_processor')
    logger.setLevel(logging.INFO)
    
    if not logger.handlers:
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    
    logger.info(f"Starting to process {csv_file} in chunks of {chunk_size}")
    
    # Initialize statistics
    total_chunks = 0
    total_rows = 0
    processed_rows = 0
    error_rows = 0
    
    try:
        # Count total rows in CSV (fast way)
        with open(csv_file, 'r') as f:
            total_rows = sum(1 for _ in f) - 1  # Subtract header row
        
        logger.info(f"Total rows in CSV: {total_rows}")
        
        # Process in chunks
        for chunk_num, chunk in enumerate(pd.read_csv(csv_file, chunksize=chunk_size)):
            total_chunks += 1
            chunk_rows = len(chunk)
            
            logger.info(f"Processing chunk {chunk_num+1} with {chunk_rows} rows")
            
            try:
                # Process chunk if a processing function is provided
                if process_func:
                    result = process_func(chunk)
                    
                    # Update statistics based on result
                    if isinstance(result, dict):
                        processed_rows += result.get('processed', chunk_rows)
                        error_rows += result.get('errors', 0)
                    else:
                        processed_rows += chunk_rows
                else:
                    # Default processing just counts rows
                    processed_rows += chunk_rows
                
                # Calculate progress
                progress = (processed_rows / total_rows) * 100
                logger.info(f"Progress: {progress:.1f}% ({processed_rows}/{total_rows} rows)")
                
            except Exception as e:
                logger.error(f"Error processing chunk {chunk_num+1}: {str(e)}")
                error_rows += chunk_rows
        
        # Calculate elapsed time
        elapsed = (datetime.now() - start_time).total_seconds()
        
        # Log results
        logger.info(f"Processing completed in {elapsed:.2f} seconds")
        logger.info(f"Chunks processed: {total_chunks}")
        logger.info(f"Rows processed: {processed_rows}")
        logger.info(f"Rows with errors: {error_rows}")
        
        return {
            'success': True,
            'total_chunks': total_chunks,
            'total_rows': total_rows,
            'processed_rows': processed_rows,
            'error_rows': error_rows,
            'elapsed_seconds': elapsed
        }
    
    except Exception as e:
        logger.error(f"Error processing CSV: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'total_chunks': total_chunks,
            'total_rows': total_rows,
            'processed_rows': processed_rows,
            'error_rows': error_rows
        }
```

## Sync Control and Monitoring

### Sync Control Table Management
```python
def create_sync_control_table(conn):
    """
    Create the sync_control table for tracking synchronization state.
    
    Args:
        conn: Database connection
        
    Returns:
        True if created successfully, False otherwise
    """
    try:
        with conn.cursor() as cursor:
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS sync_control (
                    source_table VARCHAR(255) PRIMARY KEY,
                    last_sync_time TIMESTAMP NOT NULL,
                    records_processed INTEGER NOT NULL DEFAULT 0,
                    success_count INTEGER NOT NULL DEFAULT 0,
                    error_count INTEGER NOT NULL DEFAULT 0,
                    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                    metrics JSONB
                )
            """)
        conn.commit()
        return True
    except Exception as e:
        print(f"Error creating sync_control table: {str(e)}")
        conn.rollback()
        return False

def get_sync_status(conn, table_name=None):
    """
    Get synchronization status for one or all tables.
    
    Args:
        conn: Database connection
        table_name: Optional table name to get status for
        
    Returns:
        Dictionary with sync status information
    """
    try:
        with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cursor:
            if table_name:
                cursor.execute("""
                    SELECT * FROM sync_control
                    WHERE source_table = %s
                """, (table_name,))
                result = cursor.fetchone()
                return result if result else {
                    'source_table': table_name,
                    'last_sync_time': None,
                    'records_processed': 0,
                    'success_count': 0,
                    'error_count': 0
                }
            else:
                cursor.execute("""
                    SELECT * FROM sync_control
                    ORDER BY updated_at DESC
                """)
                return cursor.fetchall()
    except Exception as e:
        print(f"Error getting sync status: {str(e)}")
        return None

def update_sync_status(conn, table_name, metrics):
    """
    Update the sync_control table with sync metrics.
    
    Args:
        conn: Database connection
        table_name: Table name
        metrics: Dictionary with sync metrics
        
    Returns:
        True if updated successfully, False otherwise
    """
    try:
        now = datetime.now()
        
        with conn.cursor() as cursor:
            # Check if record exists
            cursor.execute("""
                SELECT 1 FROM sync_control
                WHERE source_table = %s
            """, (table_name,))
            
            exists = cursor.fetchone() is not None
            
            if exists:
                # Update existing record
                cursor.execute("""
                    UPDATE sync_control
                    SET last_sync_time = %s,
                        records_processed = records_processed + %s,
                        success_count = success_count + %s,
                        error_count = error_count + %s,
                        updated_at = %s,
                        metrics = %s
                    WHERE source_table = %s
                """, (
                    now,
                    metrics.get('processed', 0),
                    metrics.get('success', metrics.get('processed', 0) - metrics.get('errors', 0)),
                    metrics.get('errors', 0),
                    now,
                    psycopg2.extras.Json(metrics),
                    table_name
                ))
            else:
                # Insert new record
                cursor.execute("""
                    INSERT INTO sync_control (
                        source_table, last_sync_time, records_processed, 
                        success_count, error_count, created_at, updated_at, metrics
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                """, (
                    table_name,
                    now,
                    metrics.get('processed', 0),
                    metrics.get('success', metrics.get('processed', 0) - metrics.get('errors', 0)),
                    metrics.get('errors', 0),
                    now,
                    now,
                    psycopg2.extras.Json(metrics)
                ))
        
        conn.commit()
        return True
    except Exception as e:
        print(f"Error updating sync status: {str(e)}")
        conn.rollback()
        return False
```

### Sync Monitoring Dashboard
```python
def generate_sync_report_html(conn):
    """
    Generate an HTML report of synchronization status.
    
    Args:
        conn: Database connection
        
    Returns:
        HTML string with sync report
    """
    try:
        # Get sync status for all tables
        with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cursor:
            cursor.execute("""
                SELECT 
                    source_table,
                    last_sync_time,
                    records_processed,
                    success_count,
                    error_count,
                    updated_at,
                    (error_count::float / NULLIF(records_processed, 0) * 100) AS error_rate
                FROM 
                    sync_control
                ORDER BY 
                    updated_at DESC
            """)
            
            sync_data = cursor.fetchall()
        
        # Generate HTML report
        html = """
        <!DOCTYPE html>
        <html>
        <head>
            <title>Data Synchronization Report</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; }
                h1 { color: #333; }
                table { border-collapse: collapse; width: 100%; }
                th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
                th { background-color: #f2f2f2; }
                tr:nth-child(even) { background-color: #f9f9f9; }
                .success { color: green; }
                .warning { color: orange; }
                .error { color: red; }
            </style>
        </head>
        <body>
            <h1>Data Synchronization Report</h1>
            <p>Generated on: """ + datetime.now().strftime('%Y-%m-%d %H:%M:%S') + """</p>
            
            <h2>Synchronization Status</h2>
            <table>
                <tr>
                    <th>Table</th>
                    <th>Last Sync</th>
                    <th>Records Processed</th>
                    <th>Success</th>
                    <th>Errors</th>
                    <th>Error Rate</th>
                    <th>Last Updated</th>
                </tr>
        """
        
        for row in sync_data:
            # Determine status class based on error rate
            status_class = 'success'
            if row['error_rate'] and row['error_rate'] > 5:
                status_class = 'error'
            elif row['error_rate'] and row['error_rate'] > 1:
                status_class = 'warning'
            
            # Format dates
            last_sync = row['last_sync_time'].strftime('%Y-%m-%d %H:%M:%S') if row['last_sync_time'] else 'Never'
            updated_at = row['updated_at'].strftime('%Y-%m-%d %H:%M:%S') if row['updated_at'] else 'Unknown'
            
            # Add row to table
            html += f"""
                <tr class="{status_class}">
                    <td>{row['source_table']}</td>
                    <td>{last_sync}</td>
                    <td>{row['records_processed']}</td>
                    <td>{row['success_count']}</td>
                    <td>{row['error_count']}</td>
                    <td>{row['error_rate']:.2f}% if row['error_rate'] else '0.00%'</td>
                    <td>{updated_at}</td>
                </tr>
            """
        
        # Complete HTML
        html += """
            </table>
        </body>
        </html>
        """
        
        return html
    
    except Exception as e:
        print(f"Error generating sync report: {str(e)}")
        return f"<html><body><h1>Error</h1><p>{str(e)}</p></body></html>"
```

### Sync Performance Analysis
```python
def analyze_sync_performance(conn):
    """
    Analyze synchronization performance across tables.
    
    Args:
        conn: Database connection
        
    Returns:
        Dictionary with performance statistics
    """
    try:
        with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cursor:
            # Get metrics from sync_control
            cursor.execute("""
                SELECT 
                    source_table,
                    metrics,
                    records_processed,
                    updated_at - created_at AS total_duration
                FROM 
                    sync_control
                WHERE 
                    metrics IS NOT NULL
            """)
            
            rows = cursor.fetchall()
        
        performance_stats = {}
        
        for row in rows:
            table_name = row['source_table']
            metrics = row['metrics']
            
            if not metrics:
                continue
            
            # Extract key metrics
            elapsed_seconds = metrics.get('elapsed_seconds', 0)
            processed = metrics.get('processed', row['records_processed'])
            
            # Calculate records per second
            records_per_second = processed / elapsed_seconds if elapsed_seconds > 0 else 0
            
            # Store statistics
            performance_stats[table_name] = {
                'records_processed': processed,
                'elapsed_seconds': elapsed_seconds,
                'records_per_second': records_per_second,
                'elapsed_formatted': format_time_duration(elapsed_seconds)
            }
        
        return performance_stats
    
    except Exception as e:
        print(f"Error analyzing sync performance: {str(e)}")
        return {}

def format_time_duration(seconds):
    """Format seconds into a human-readable duration."""
    if seconds < 60:
        return f"{seconds:.1f} seconds"
    elif seconds < 3600:
        minutes = seconds / 60
        return f"{minutes:.1f} minutes"
    else:
        hours = seconds / 3600
        return f"{hours:.1f} hours"
```

## ID Mapping and Preservation

### ID Mapping Table Management
```python
def create_id_mapping_table(conn, mapping_table_name, source_type, target_type):
    """
    Create a table for mapping IDs between systems.
    
    Args:
        conn: Database connection
        mapping_table_name: Name for the mapping table
        source_type: Data type for source IDs
        target_type: Data type for target IDs
        
    Returns:
        True if created successfully, False otherwise
    """
    try:
        with conn.cursor() as cursor:
            cursor.execute(f"""
                CREATE TABLE IF NOT EXISTS {mapping_table_name} (
                    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                    source_id {source_type} NOT NULL,
                    target_id {target_type} NOT NULL,
                    source_system VARCHAR(50) NOT NULL,
                    target_system VARCHAR(50) NOT NULL,
                    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                    metadata JSONB,
                    UNIQUE(source_id, source_system, target_system)
                )
            """)
            
            # Create indexes for efficient lookups
            cursor.execute(f"""
                CREATE INDEX IF NOT EXISTS idx_{mapping_table_name}_source
                ON {mapping_table_name} (source_id, source_system)
            """)
            
            cursor.execute(f"""
                CREATE INDEX IF NOT EXISTS idx_{mapping_table_name}_target
                ON {mapping_table_name} (target_id, target_system)
            """)
        
        conn.commit()
        return True
    
    except Exception as e:
        print(f"Error creating ID mapping table: {str(e)}")
        conn.rollback()
        return False

def add_id_mapping(conn, mapping_table_name, source_id, target_id, 
                 source_system, target_system, metadata=None):
    """
    Add a mapping between source and target IDs.
    
    Args:
        conn: Database connection
        mapping_table_name: Name of the mapping table
        source_id: ID in the source system
        target_id: ID in the target system
        source_system: Name of the source system
        target_system: Name of the target system
        metadata: Optional JSON metadata
        
    Returns:
        True if added successfully, False otherwise
    """
    try:
        with conn.cursor() as cursor:
            cursor.execute(f"""
                INSERT INTO {mapping_table_name}
                    (source_id, target_id, source_system, target_system, metadata)
                VALUES
                    (%s, %s, %s, %s, %s)
                ON CONFLICT (source_id, source_system, target_system)
                DO UPDATE SET
                    target_id = EXCLUDED.target_id,
                    updated_at = CURRENT_TIMESTAMP,
                    metadata = COALESCE(EXCLUDED.metadata, {mapping_table_name}.metadata)
            """, (
                source_id,
                target_id,
                source_system,
                target_system,
                psycopg2.extras.Json(metadata) if metadata else None
            ))
        
        conn.commit()
        return True
    
    except Exception as e:
        print(f"Error adding ID mapping: {str(e)}")
        conn.rollback()
        return False

def get_id_mapping(conn, mapping_table_name, source_id, source_system, target_system):
    """
    Get target ID for a given source ID.
    
    Args:
        conn: Database connection
        mapping_table_name: Name of the mapping table
        source_id: ID in the source system
        source_system: Name of the source system
        target_system: Name of the target system
        
    Returns:
        Target ID if found, None otherwise
    """
    try:
        with conn.cursor() as cursor:
            cursor.execute(f"""
                SELECT target_id FROM {mapping_table_name}
                WHERE source_id = %s
                AND source_system = %s
                AND target_system = %s
            """, (source_id, source_system, target_system))
            
            result = cursor.fetchone()
            return result[0] if result else None
    
    except Exception as e:
        print(f"Error getting ID mapping: {str(e)}")
        return None
```

### Bulk ID Mapping Operations
```python
def bulk_add_id_mappings(conn, mapping_table_name, mappings, batch_size=1000):
    """
    Add multiple ID mappings in bulk.
    
    Args:
        conn: Database connection
        mapping_table_name: Name of the mapping table
        mappings: List of mapping dictionaries with source_id, target_id, etc.
        batch_size: Number of mappings to add in each batch
        
    Returns:
        Number of mappings added
    """
    try:
        with conn.cursor() as cursor:
            # Prepare insert query
            insert_query = f"""
                INSERT INTO {mapping_table_name}
                    (source_id, target_id, source_system, target_system, metadata)
                VALUES
                    (%s, %s, %s, %s, %s)
                ON CONFLICT (source_id, source_system, target_system)
                DO UPDATE SET
                    target_id = EXCLUDED.target_id,
                    updated_at = CURRENT_TIMESTAMP,
                    metadata = COALESCE(EXCLUDED.metadata, {mapping_table_name}.metadata)
            """
            
            # Process in batches
            total_added = 0
            for i in range(0, len(mappings), batch_size):
                batch = mappings[i:i+batch_size]
                
                # Prepare batch data
                batch_data = []
                for mapping in batch:
                    batch_data.append((
                        mapping['source_id'],
                        mapping['target_id'],
                        mapping['source_system'],
                        mapping['target_system'],
                        psycopg2.extras.Json(mapping.get('metadata')) if mapping.get('metadata') else None
                    ))
                
                # Execute batch insert
                execute_batch(cursor, insert_query, batch_data)
                total_added += len(batch)
                
                # Log progress
                print(f"Added {total_added}/{len(mappings)} mappings")
        
        conn.commit()
        return total_added
    
    except Exception as e:
        print(f"Error adding ID mappings in bulk: {str(e)}")
        conn.rollback()
        return 0

def build_mapping_from_tables(conn, mapping_table_name, source_table, source_id_field,
                            target_table, target_id_field, join_field, 
                            source_system, target_system):
    """
    Build ID mappings from two tables that share a common field.
    
    Args:
        conn: Database connection
        mapping_table_name: Name of the mapping table
        source_table: Name of the source table
        source_id_field: Name of the ID field in the source table
        target_table: Name of the target table
        target_id_field: Name of the ID field in the target table
        join_field: Name of the common field to join on
        source_system: Name of the source system
        target_system: Name of the target system
        
    Returns:
        Number of mappings created
    """
    try:
        with conn.cursor() as cursor:
            # Create mappings from source and target tables
            cursor.execute(f"""
                INSERT INTO {mapping_table_name}
                    (source_id, target_id, source_system, target_system)
                SELECT 
                    s.{source_id_field} AS source_id,
                    t.{target_id_field} AS target_id,
                    %s AS source_system,
                    %s AS target_system
                FROM 
                    {source_table} s
                JOIN 
                    {target_table} t ON s.{join_field} = t.{join_field}
                ON CONFLICT (source_id, source_system, target_system)
                DO UPDATE SET
                    target_id = EXCLUDED.target_id,
                    updated_at = CURRENT_TIMESTAMP
            """, (source_system, target_system))
            
            # Get the number of rows affected
            mappings_created = cursor.rowcount
        
        conn.commit()
        return mappings_created
    
    except Exception as e:
        print(f"Error building ID mappings from tables: {str(e)}")
        conn.rollback()
        return 0
```

## Synchronization Error Handling

### Error Logging and Recovery
```python
def log_sync_error(conn, error_table_name, source_table, record_id, error_type, 
                  error_message, record_data=None):
    """
    Log a synchronization error for later recovery.
    
    Args:
        conn: Database connection
        error_table_name: Name of the error logging table
        source_table: Name of the source table
        record_id: ID of the record that failed
        error_type: Type of error
        error_message: Error message
        record_data: Optional JSON representation of the record
        
    Returns:
        True if logged successfully, False otherwise
    """
    try:
        with conn.cursor() as cursor:
            # Create error logging table if it doesn't exist
            cursor.execute(f"""
                CREATE TABLE IF NOT EXISTS {error_table_name} (
                    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                    source_table VARCHAR(255) NOT NULL,
                    record_id VARCHAR(255) NOT NULL,
                    error_type VARCHAR(255) NOT NULL,
                    error_message TEXT NOT NULL,
                    record_data JSONB,
                    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                    resolved BOOLEAN NOT NULL DEFAULT FALSE,
                    resolved_at TIMESTAMP,
                    resolution_notes TEXT
                )
            """)
            
            # Log the error
            cursor.execute(f"""
                INSERT INTO {error_table_name}
                    (source_table, record_id, error_type, error_message, record_data)
                VALUES
                    (%s, %s, %s, %s, %s)
            """, (
                source_table,
                str(record_id),
                error_type,
                error_message,
                psycopg2.extras.Json(record_data) if record_data else None
            ))
        
        conn.commit()
        return True
    
    except Exception as e:
        print(f"Error logging sync error: {str(e)}")
        conn.rollback()
        return False

def retry_failed_records(conn, error_table_name, process_func, limit=100):
    """
    Retry processing records that previously failed.
    
    Args:
        conn: Database connection
        error_table_name: Name of the error logging table
        process_func: Function to process a record (takes record_data as input)
        limit: Maximum number of records to retry
        
    Returns:
        Dictionary with retry statistics
    """
    try:
        # Get unresolved errors
        with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cursor:
            cursor.execute(f"""
                SELECT id, source_table, record_id, error_type, record_data
                FROM {error_table_name}
                WHERE resolved = FALSE
                ORDER BY created_at
                LIMIT {limit}
            """)
            
            errors = cursor.fetchall()
        
        # Process each error
        total = len(errors)
        success = 0
        failed = 0
        
        for error in errors:
            try:
                # Skip records without data
                if not error['record_data']:
                    print(f"No record data for error {error['id']}, marking as resolved")
                    with conn.cursor() as cursor:
                        cursor.execute(f"""
                            UPDATE {error_table_name}
                            SET resolved = TRUE,
                                resolved_at = CURRENT_TIMESTAMP,
                                resolution_notes = 'No record data available'
                            WHERE id = %s
                        """, (error['id'],))
                    conn.commit()
                    continue
                
                # Process the record
                result = process_func(error['record_data'])
                
                # Mark as resolved if successful
                with conn.cursor() as cursor:
                    cursor.execute(f"""
                        UPDATE {error_table_name}
                        SET resolved = TRUE,
                            resolved_at = CURRENT_TIMESTAMP,
                            resolution_notes = %s
                        WHERE id = %s
                    """, (
                        f"Successfully reprocessed: {result}",
                        error['id']
                    ))
                
                conn.commit()
                success += 1
                
                print(f"Successfully reprocessed error {error['id']}")
                
            except Exception as e:
                # Update error with new error message
                with conn.cursor() as cursor:
                    cursor.execute(f"""
                        UPDATE {error_table_name}
                        SET error_message = %s,
                            created_at = CURRENT_TIMESTAMP
                        WHERE id = %s
                    """, (
                        f"Retry failed: {str(e)}",
                        error['id']
                    ))
                
                conn.commit()
                failed += 1
                
                print(f"Failed to reprocess error {error['id']}: {str(e)}")
        
        return {
            'total': total,
            'success': success,
            'failed': failed
        }
    
    except Exception as e:
        print(f"Error retrying failed records: {str(e)}")
        return {
            'total': 0,
            'success': 0,
            'failed': 0,
            'error': str(e)
        }
```

### Transaction Management with Rollback Points
```python
def process_with_savepoints(conn, records, process_func, batch_size=100):
    """
    Process records with savepoints for partial rollback on errors.
    
    Args:
        conn: Database connection
        records: List of records to process
        process_func: Function to process a record
        batch_size: Number of records to process in each batch
        
    Returns:
        Dictionary with processing statistics
    """
    total = len(records)
    processed = 0
    success = 0
    failed = 0
    
    try:
        # Start transaction
        with conn.cursor() as cursor:
            cursor.execute("BEGIN")
            
            # Process in batches
            for i in range(0, total, batch_size):
                batch = records[i:i+batch_size]
                batch_size = len(batch)
                
                print(f"Processing batch {i//batch_size + 1} with {batch_size} records")
                
                # Process each record with a savepoint
                for j, record in enumerate(batch):
                    # Create savepoint for this record
                    savepoint_name = f"sp_{i + j}"
                    cursor.execute(f"SAVEPOINT {savepoint_name}")
                    
                    try:
                        # Process the record
                        process_func(cursor, record)
                        
                        # Release savepoint on success
                        cursor.execute(f"RELEASE SAVEPOINT {savepoint_name}")
                        success += 1
                    
                    except Exception as e:
                        # Rollback to savepoint on error
                        cursor.execute(f"ROLLBACK TO SAVEPOINT {savepoint_name}")
                        failed += 1
                        
                        print(f"Error processing record {i + j}: {str(e)}")
                        
                        # Log the error
                        log_sync_error(
                            conn, 
                            "sync_errors", 
                            "batch_processing", 
                            str(i + j), 
                            "processing_error", 
                            str(e), 
                            record
                        )
                    
                    processed += 1
                    
                    # Log progress periodically
                    if processed % 100 == 0:
                        print(f"Processed {processed}/{total} records "
                             f"({success} success, {failed} failed)")
            
            # Commit transaction
            conn.commit()
    
    except Exception as e:
        # Rollback entire transaction on critical error
        conn.rollback()
        print(f"Critical error during processing: {str(e)}")
    
    return {
        'total': total,
        'processed': processed,
        'success': success,
        'failed': failed
    }
```